#include "flipud_metax.h"
#include "../../../devices/metax/metax_common.h"
#include "../../../devices/metax/metax_handle.h"
#include <mcr/mc_runtime.h>
#include <cstdint>
#include <algorithm>
#include <vector>
#include <maca_fp16.h>
#include <maca_bfloat16.h>

namespace op::flipud::metax {

constexpr int MAX_DIMS = 4;

struct TensorLayout {
    int ndim;
    int shape[MAX_DIMS];
    int in_strides[MAX_DIMS];
    int out_strides[MAX_DIMS];
};

template <int Bytes> struct VectorType;
template <> struct VectorType<16> { using type = int4; }; // 128-bit
template <> struct VectorType<8>  { using type = int2; }; // 64-bit
template <> struct VectorType<4>  { using type = int; };  // 32-bit

// ---------------------------------------------------------

__device__ inline size_t get_offset(int idx, const int* strides, int ndim, const int* shape) {
    size_t offset = 0;
    int rem = idx;
    #pragma unroll
    for (int i = ndim - 1; i >= 0; --i) {
        int dim_sz = shape[i];
        int pos = rem % dim_sz;
        rem /= dim_sz;
        offset += pos * strides[i];
    }
    return offset;
}

__device__ inline size_t get_flipud_src_offset(int idx, const int* strides, int ndim, const int* shape) {
    size_t offset = 0;
    int rem = idx;
    #pragma unroll
    for (int i = ndim - 1; i >= 0; --i) {
        int dim_sz = shape[i];
        int pos = rem % dim_sz;
        rem /= dim_sz;
        
        if (i == 0) {
            pos = dim_sz - 1 - pos;
        }
        offset += pos * strides[i];
    }
    return offset;
}

// 标量 Kernel
template <typename T>
__global__ void flipud_kernel(
    T* dst, const T* src, size_t n, TensorLayout layout) 
{
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= n) return;

    size_t dst_off = get_offset(idx, layout.out_strides, layout.ndim, layout.shape);
    size_t src_off = get_flipud_src_offset(idx, layout.in_strides, layout.ndim, layout.shape);

    dst[dst_off] = src[src_off];
}

// 向量化 Kernel
template <typename T, int PackSize>
__global__ void flipud_kernel_vectorized(
    T* dst, const T* src, size_t num_packs, TensorLayout layout) 
{
    // [修正] 使用 int4/int2 替代 aligned_storage
    using VecT = typename VectorType<sizeof(T) * PackSize>::type;
    
    size_t pack_idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (pack_idx >= num_packs) return;
    
    int strides_in[MAX_DIMS], strides_out[MAX_DIMS], shape[MAX_DIMS];
    
    #pragma unroll
    for(int i=0; i<MAX_DIMS; ++i) {
        shape[i] = layout.shape[i];
        
        // [逻辑正确] 确保所有步长都除以 PackSize
        if (i == layout.ndim - 1) {
            strides_in[i] = 1;
            strides_out[i] = 1;
        } else {
            strides_in[i] = layout.in_strides[i] / PackSize;
            strides_out[i] = layout.out_strides[i] / PackSize;
        }
    }
    
    if (layout.ndim > 0) {
        shape[layout.ndim-1] /= PackSize; 
    }

    size_t dst_pack_off = get_offset(pack_idx, strides_out, layout.ndim, shape);
    size_t src_pack_off = get_flipud_src_offset(pack_idx, strides_in, layout.ndim, shape);

    // 强转为向量类型进行读写
    const VecT* src_vec = reinterpret_cast<const VecT*>(src);
    VecT* dst_vec = reinterpret_cast<VecT*>(dst);

    dst_vec[dst_pack_off] = src_vec[src_pack_off];
}

static inline bool is_pointer_aligned(const void *ptr, size_t alignment) {
    return reinterpret_cast<uintptr_t>(ptr) % alignment == 0;
}

struct Descriptor::Opaque {
    TensorLayout layout;
};

template <typename T>
void launch_kernel(
    void *output, const void *input,
    TensorLayout layout,
    size_t numel,
    void *stream) {

    auto in_ptr = reinterpret_cast<const T *>(input);
    auto out_ptr = reinterpret_cast<T *>(output);
    auto mc_stream = reinterpret_cast<mcStream_t>(stream);
    
    constexpr int TotalBytes = 16; 
    constexpr int PackSize = TotalBytes / sizeof(T);
    
    // ---------------- Check Vectorization ----------------
    bool is_ptr_aligned = is_pointer_aligned(output, TotalBytes) && is_pointer_aligned(input, TotalBytes);
    bool is_numel_divisible = (numel % PackSize == 0);
    bool is_last_dim_aligned = (layout.ndim > 0) && (layout.shape[layout.ndim-1] % PackSize == 0);

    bool is_inner_contiguous = false;
    if (layout.ndim > 0) {
        if (layout.in_strides[layout.ndim-1] == 1 && layout.out_strides[layout.ndim-1] == 1) {
            is_inner_contiguous = true;
        }
    }
    
    bool is_stride_aligned = true;
    for (int i = 0; i < layout.ndim - 1; ++i) {
        if (layout.in_strides[i] % PackSize != 0 || layout.out_strides[i] % PackSize != 0) {
            is_stride_aligned = false;
            break;
        }
    }

    // [逻辑正确] 1D Tensor 禁止向量化
    bool is_dim_safe = (layout.ndim > 1);

    bool can_vectorize = (PackSize > 1) && 
                         is_ptr_aligned &&
                         is_numel_divisible &&
                         is_last_dim_aligned &&
                         is_inner_contiguous &&
                         is_stride_aligned &&
                         is_dim_safe;

    if (can_vectorize) {
        size_t num_packs = numel / PackSize;
        size_t block_size = 256;
        size_t grid_size = (num_packs + block_size - 1) / block_size;
        
        flipud_kernel_vectorized<T, PackSize>
            <<<grid_size, block_size, 0, mc_stream>>>(out_ptr, in_ptr, num_packs, layout);
    } else {
        size_t block_size = 256;
        size_t grid_size = (numel + block_size - 1) / block_size;
        
        flipud_kernel<T>
            <<<grid_size, block_size, 0, mc_stream>>>(out_ptr, in_ptr, numel, layout);
    }
}

Descriptor::~Descriptor() { 
    if (_opaque) delete _opaque; 
}

infiniStatus_t Descriptor::create(
    infiniopHandle_t handle_, Descriptor **desc_ptr,
    infiniopTensorDescriptor_t out_desc, infiniopTensorDescriptor_t input_desc) {

    auto handle = reinterpret_cast<device::metax::Handle *>(handle_);
    auto info_result = FlipudInfo::create(out_desc, input_desc);
    if (!info_result) return info_result.status();

    auto opaque = new Opaque();
    opaque->layout.ndim = static_cast<int>(input_desc->ndim());
    
    if (opaque->layout.ndim > MAX_DIMS) {
        delete opaque;
        return INFINI_STATUS_BAD_TENSOR_SHAPE;
    }

    const auto& shape = input_desc->shape();
    const auto& in_strides = input_desc->strides();
    const auto& out_strides = out_desc->strides();

    for (int i = 0; i < opaque->layout.ndim; ++i) {
        opaque->layout.shape[i] = shape[i];
        opaque->layout.in_strides[i] = in_strides[i];
        opaque->layout.out_strides[i] = out_strides[i];
    }

    *desc_ptr = new Descriptor(opaque, info_result.take(), 0, handle->device, handle->device_id);
    return INFINI_STATUS_SUCCESS;
}

infiniStatus_t Descriptor::calculate(
    void *workspace, size_t workspace_size, void *output,
    const void *input, void *stream) const {

    auto dtype = _info.dtype();
    auto numel = _info.numel();

    switch (dtype) {
    case INFINI_DTYPE_F16:
        launch_kernel<__half>(output, input, _opaque->layout, numel, stream);
        break;
    case INFINI_DTYPE_BF16:
        launch_kernel<__maca_bfloat16>(output, input, _opaque->layout, numel, stream);
        break;
    case INFINI_DTYPE_F32:
        launch_kernel<float>(output, input, _opaque->layout, numel, stream);
        break;
    case INFINI_DTYPE_F64:
        launch_kernel<double>(output, input, _opaque->layout, numel, stream);
        break;
    case INFINI_DTYPE_I32:
        launch_kernel<int>(output, input, _opaque->layout, numel, stream);
        break;
    default:
        return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }

    return INFINI_STATUS_SUCCESS;
}

} // namespace op::flipud::metax