#include "../../../devices/metax/metax_common.h"
#include "../../../devices/metax/metax_kernel_common.h"
#include "../paged_caching/cuda/kernel.cuh"
#include "reshape_and_cache_metax.h"

template <typename Tdata, int NUM_THREADS>
INFINIOP_METAX_KERNEL reshapeAndCache(
    Tdata *k_cache, Tdata *v_cache,
    const Tdata *k, const Tdata *v,
    const int64_t *slot_mapping,
    const size_t head_size, const size_t block_size,
    const ptrdiff_t k_src_stride, const ptrdiff_t v_src_stride,
    const ptrdiff_t k_cache_block_stride, const ptrdiff_t v_cache_block_stride) {
    op::paged_caching::cuda::pagedCachingKernel<Tdata, NUM_THREADS>(
        k_cache, v_cache, k, v, slot_mapping, head_size,
        block_size, k_src_stride, v_src_stride, k_cache_block_stride, v_cache_block_stride);
}

namespace op::reshape_and_cache::metax {

struct Descriptor::Opaque {
    std::shared_ptr<device::metax::Handle::Internal> internal;
};

Descriptor::~Descriptor() {
    delete _opaque;
}

infiniStatus_t Descriptor::create(
    infiniopHandle_t handle,
    Descriptor **desc_ptr,
    infiniopTensorDescriptor_t key_desc,
    infiniopTensorDescriptor_t value_desc,
    infiniopTensorDescriptor_t key_cache_desc,
    infiniopTensorDescriptor_t value_cache_desc,
    infiniopTensorDescriptor_t slot_mapping_desc,
    const char *kv_cache_dtype) {

    (void)kv_cache_dtype;
    auto info = ReshapeAndCacheInfo::create(
        key_desc, value_desc, key_cache_desc, value_cache_desc, slot_mapping_desc, kv_cache_dtype);
    CHECK_RESULT(info);

    *desc_ptr = new Descriptor(
        new Opaque{reinterpret_cast<device::metax::Handle *>(handle)->internal()},
        info.take(), 0, handle->device, handle->device_id);

    return INFINI_STATUS_SUCCESS;
}

template <int NUM_THREADS>
infiniStatus_t launchKernel(const ReshapeAndCacheInfo &info,
                            void *k_cache, void *v_cache,
                            infiniDtype_t dtype,
                            const void *k, const void *v,
                            const void *slot_mapping,
                            size_t num_tokens, size_t num_kv_heads, size_t head_size, size_t block_size,
                            ptrdiff_t k_src_stride, ptrdiff_t v_src_stride,
                            ptrdiff_t k_cache_block_stride, ptrdiff_t v_cache_block_stride,
                            hcStream_t stream) {

    dim3 grid(uint64_t(num_kv_heads), uint64_t(num_tokens), 1);
    dim3 block(NUM_THREADS);
    size_t shared_mem_size = 0;

    if (dtype == INFINI_DTYPE_F16) {
        reshapeAndCache<half, NUM_THREADS>
            <<<grid, block, shared_mem_size, stream>>>(
                (half *)k_cache,
                (half *)v_cache,
                (const half *)k,
                (const half *)v,
                (const int64_t *)slot_mapping,
                head_size,
                block_size,
                k_src_stride,
                v_src_stride,
                k_cache_block_stride,
                v_cache_block_stride);
    } else if (dtype == INFINI_DTYPE_BF16) {
        reshapeAndCache<cuda_bfloat16, NUM_THREADS>
            <<<grid, block, shared_mem_size, stream>>>(
                (cuda_bfloat16 *)k_cache,
                (cuda_bfloat16 *)v_cache,
                (const cuda_bfloat16 *)k,
                (const cuda_bfloat16 *)v,
                (const int64_t *)slot_mapping,
                head_size,
                block_size,
                k_src_stride,
                v_src_stride,
                k_cache_block_stride,
                v_cache_block_stride);
    } else if (dtype == INFINI_DTYPE_F32) {
        reshapeAndCache<float, NUM_THREADS>
            <<<grid, block, shared_mem_size, stream>>>(
                (float *)k_cache,
                (float *)v_cache,
                (const float *)k,
                (const float *)v,
                (const int64_t *)slot_mapping,
                head_size,
                block_size,
                k_src_stride,
                v_src_stride,
                k_cache_block_stride,
                v_cache_block_stride);
    } else {
        return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }
    return INFINI_STATUS_SUCCESS;
}

infiniStatus_t Descriptor::calculate(
    void *workspace,
    size_t workspace_size,
    void *key,
    void *value,
    void *key_cache,
    void *value_cache,
    const void *slot_mapping,
    const char *kv_cache_dtype,
    void *k_scale,
    void *v_scale,
    void *stream_) const {

    (void)workspace;
    (void)workspace_size;
    (void)kv_cache_dtype;
    (void)k_scale;
    (void)v_scale;

    hcStream_t stream = (hcStream_t)stream_;

    int max_threads = _opaque->internal->maxThreadsPerBlock();
    if (max_threads >= METAX_BLOCK_SIZE_1024) {
        launchKernel<METAX_BLOCK_SIZE_1024>(
            _info, key_cache, value_cache, _info.dtype, key, value, slot_mapping,
            _info.num_tokens, _info.num_kv_heads, _info.head_size, _info.block_size,
            _info.k_src_stride, _info.v_src_stride,
            _info.k_cache_block_stride, _info.v_cache_block_stride,
            stream);
    } else if (max_threads >= METAX_BLOCK_SIZE_512) {
        launchKernel<METAX_BLOCK_SIZE_512>(
            _info, key_cache, value_cache, _info.dtype, key, value, slot_mapping,
            _info.num_tokens, _info.num_kv_heads, _info.head_size, _info.block_size,
            _info.k_src_stride, _info.v_src_stride,
            _info.k_cache_block_stride, _info.v_cache_block_stride,
            stream);
    } else {
        return INFINI_STATUS_DEVICE_ARCHITECTURE_NOT_SUPPORTED;
    }

    return INFINI_STATUS_SUCCESS;
}

} // namespace op::reshape_and_cache::metax
