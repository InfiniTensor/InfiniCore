// https://github.com/Oneflow-Inc/oneflow/blob/25c8978c1c8b1371ef6aa4187dae4495bd233c35/oneflow/user/kernels/variance_kernel_util.cu#L90
namespace {
    template<typename T, typename ComputeType>
    inline __device__ void WelfordReduce(const T* in_ptr, ComputeType* mean, ComputeType* m2,
                                         ComputeType* count, const size_t total_elem_cnt,
                                         const size_t start, const size_t step) {
      ComputeType old_mean = 0.0;
      for (size_t i = start; i < total_elem_cnt; i += step) {
        ++(*count);
        old_mean = *mean;
        *mean += (static_cast<ComputeType>(in_ptr[i]) - *mean) / *count;
        *m2 += (static_cast<ComputeType>(in_ptr[i]) - *mean)
               * (static_cast<ComputeType>(in_ptr[i]) - old_mean);
      }
    }
    
    template<typename T>
    inline __device__ void WelfordCombine(const T* b_mean, const T* b_m2, const T* b_count, T* mean,
                                          T* m2, T* count, const size_t total_elem_cnt,
                                          const size_t start, const size_t step) {
      for (size_t i = start; i < total_elem_cnt; i += step) {
        cuda::layer_norm::WelfordCombine(b_mean[i], b_m2[i], b_count[i], mean, m2, count);
      }
    }
    __device__ int32_t done_block_count = 0;
}  // namespace
    
    template<typename T, typename ComputeType>
    __global__ void ComputeVarScalarOut(const T* in_ptr, T* out_ptr, ComputeType* tmp_buffer_ptr,
                                        const VarParam var_param, bool is_nan) {
      if (is_nan) {
        if (blockIdx.x == 0 && threadIdx.x == 0) { *out_ptr = Nan<T>(); }
        return;
      }
      const size_t elems_per_block = var_param.elem_cnt / gridDim.x;
      const size_t elems_per_thread = elems_per_block / blockDim.x;
      // tail element number in block
      size_t tail_elems = elems_per_block % blockDim.x;
      // elems_per_block = (128 +1024)
      // tail_elems = (128 + 1024) % 512 = 128
      //  elems_per_block - tail_elems, threadIdx.x, blockDim.x
      //  (128 + 1024) - 128 = 1024, threadIdx.x, 512   // 一个线程处理两个元素的var mean if (elems_per_thread > 0)
      ComputeType thread_mean = 0.0;
      ComputeType thread_m2 = 0.0;
      ComputeType thread_count = 0.0;
      // every thread deal it's elems
      if (elems_per_thread > 0) { 
        const size_t block_offset = blockIdx.x * elems_per_block;
        WelfordReduce<T, ComputeType>(&in_ptr[block_offset], &thread_mean, &thread_m2, &thread_count,
                                      elems_per_block - tail_elems, threadIdx.x, blockDim.x);
      }
      // thread 0 of last block handles tail element between blocks
      if (blockIdx.x == gridDim.x - 1 && threadIdx.x == 0) {
        tail_elems += var_param.elem_cnt % gridDim.x;
      }
      // thread 0 deal tail elems
      if (tail_elems != 0 && threadIdx.x == 0) {
        const size_t tail_offset = blockIdx.x * elems_per_block + blockDim.x * elems_per_thread;
        WelfordReduce<T, ComputeType>(&in_ptr[tail_offset], &thread_mean, &thread_m2, &thread_count,
                                      tail_elems,
                                      /*tail start=*/0, /*step=*/1);
      }
    
      ComputeType block_mean = 0;
      ComputeType block_m2 = 0;
      ComputeType block_count = 0;
      cuda::layer_norm::WelfordBlockAllReduce<ComputeType>(thread_mean, thread_m2, thread_count,
                                                           &block_mean, &block_m2, &block_count);
    
      if (gridDim.x == 1) {
        if (threadIdx.x == 0) {
          *out_ptr =
              cuda::layer_norm::Div(block_m2, (var_param.unbiased ? block_count - 1 : block_count));
        }
        return;
      }
    
      ComputeType* tmp_mean_ptr = tmp_buffer_ptr;
      ComputeType* tmp_m2_ptr = &tmp_mean_ptr[gridDim.x];
      ComputeType* tmp_count_ptr = &tmp_m2_ptr[gridDim.x];
      if (threadIdx.x == 0) {
        tmp_mean_ptr[blockIdx.x] = block_mean;
        tmp_m2_ptr[blockIdx.x] = block_m2;
        tmp_count_ptr[blockIdx.x] = block_count;
      }
      __shared__ bool is_last_block;
      if (threadIdx.x == 0) { is_last_block = atomicAdd(&done_block_count, 1) == gridDim.x - 1; }
      __syncthreads();
      if (is_last_block) {
        ComputeType last_block_thread_mean = 0;
        ComputeType last_block_thread_m2 = 0;
        ComputeType last_block_thread_count = 0;
        const size_t welforddatas_per_thread = gridDim.x / blockDim.x;
        const size_t tail_welforddatas = gridDim.x % blockDim.x;
    
        if (welforddatas_per_thread > 0) {
          WelfordCombine(tmp_mean_ptr, tmp_m2_ptr, tmp_count_ptr, &last_block_thread_mean,
                         &last_block_thread_m2, &last_block_thread_count, gridDim.x - tail_welforddatas,
                         threadIdx.x, blockDim.x);
        }
        // thread 0 deal tail welford data
        if (tail_welforddatas != 0 && threadIdx.x == 0) {
          const size_t last_block_tail_offset = blockDim.x * welforddatas_per_thread;
          WelfordCombine(&tmp_mean_ptr[last_block_tail_offset], &tmp_m2_ptr[last_block_tail_offset],
                         &tmp_count_ptr[last_block_tail_offset], &last_block_thread_mean,
                         &last_block_thread_m2, &last_block_thread_count, tail_welforddatas,
                         /*tail start=*/0, /*step=*/1);
        }
        ComputeType final_mean = 0;
        ComputeType final_m2 = 0;
        ComputeType final_count = 0;
        cuda::layer_norm::WelfordBlockAllReduce<ComputeType>(
            last_block_thread_mean, last_block_thread_m2, last_block_thread_count, &final_mean,
            &final_m2, &final_count);
        if (threadIdx.x == 0) {
          *out_ptr =
              cuda::layer_norm::Div(final_m2, (var_param.unbiased ? final_count - 1 : final_count));
          done_block_count = 0;
        }
      }
    }
    






// https://github.com/Oneflow-Inc/oneflow/blob/25c8978c1c8b1371ef6aa4187dae4495bd233c35/oneflow/core/cuda/layer_norm.cuh#L259

template<typename T>
inline __device__ void WelfordCombine(T val, T* mean, T* m2, T* count) {
  // Use Welford Online algorithem to compute mean and variance
  // For more details you can refer to:
  // https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm
  *count += 1;
  T delta1 = val - *mean;
  *mean += Div(delta1, *count);
  T delta2 = val - *mean;
  *m2 += delta1 * delta2;
}

template<typename T>
inline __device__ void WelfordCombine(T b_mean, T b_m2, T b_count, T* mean, T* m2, T* count) {
  if (b_count == 0) { return; }
  T new_count = *count + b_count;
  T nb_over_n = Div(b_count, new_count);
  T delta = b_mean - *mean;
  *mean += delta * nb_over_n;
  *m2 += b_m2 + delta * delta * (*count) * nb_over_n;
  *count = new_count;
}

template<typename T, int thread_group_width = kWarpSize>
__inline__ __device__ void WelfordWarpReduce(T thread_mean, T thread_m2, T thread_count, T* mean,
                                             T* m2, T* count) {
  *mean = thread_mean;
  *m2 = thread_m2;
  *count = thread_count;
  for (int mask = thread_group_width / 2; mask > 0; mask /= 2) {
    T b_mean = __shfl_down_sync(0xffffffff, *mean, mask, thread_group_width);
    T b_m2 = __shfl_down_sync(0xffffffff, *m2, mask, thread_group_width);
    T b_count = __shfl_down_sync(0xffffffff, *count, mask, thread_group_width);
    WelfordCombine(b_mean, b_m2, b_count, mean, m2, count);
  }
}

template<typename T, int thread_group_width = kWarpSize>
__inline__ __device__ void WelfordWarpAllReduce(T thread_mean, T thread_m2, T thread_count, T* mean,
                                                T* m2, T* count) {
  WelfordWarpReduce<T, thread_group_width>(thread_mean, thread_m2, thread_count, mean, m2, count);
  *mean = __shfl_sync(0xffffffff, *mean, 0, thread_group_width);
  *m2 = __shfl_sync(0xffffffff, *m2, 0, thread_group_width);
  *count = __shfl_sync(0xffffffff, *count, 0, thread_group_width);
}

template<typename T>
__inline__ __device__ void WelfordBlockAllReduce(T thread_mean, T thread_m2, T thread_count,
                                                 T* result_mean, T* result_m2, T* result_count) {
  __shared__ T mean_shared[kWarpSize];
  __shared__ T m2_shared[kWarpSize];
  __shared__ T count_shared[kWarpSize];
  __shared__ T mean_result_broadcast;
  __shared__ T m2_result_broadcast;
  __shared__ T count_result_broadcast;
  const int lid = threadIdx.x % kWarpSize;
  const int wid = threadIdx.x / kWarpSize;
  T warp_mean = 0;
  T warp_m2 = 0;
  T warp_count = 0;
  WelfordWarpReduce(thread_mean, thread_m2, thread_count, &warp_mean, &warp_m2, &warp_count);
  __syncthreads();
  if (lid == 0) {
    mean_shared[wid] = warp_mean;
    m2_shared[wid] = warp_m2;
    count_shared[wid] = warp_count;
  }
  __syncthreads();
  if (wid == 0) {
    if (threadIdx.x < blockDim.x / kWarpSize) {
      warp_mean = mean_shared[lid];
      warp_m2 = m2_shared[lid];
      warp_count = count_shared[lid];
    } else {
      warp_mean = static_cast<T>(0);
      warp_m2 = static_cast<T>(0);
      warp_count = static_cast<T>(0);
    }
    __syncwarp();
    T block_mean = 0;
    T block_m2 = 0;
    T block_count = 0;
    WelfordWarpReduce(warp_mean, warp_m2, warp_count, &block_mean, &block_m2, &block_count);
    if (lid == 0) {
      mean_result_broadcast = block_mean;
      m2_result_broadcast = block_m2;
      count_result_broadcast = block_count;
    }
  }
  __syncthreads();
  *result_mean = mean_result_broadcast;
  *result_m2 = m2_result_broadcast;
  *result_count = count_result_broadcast;
}