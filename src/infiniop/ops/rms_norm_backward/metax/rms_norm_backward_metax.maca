#include "../../../devices/metax/metax_handle.h"
#include "../../../devices/metax/metax_common.h"
#include "../../../devices/metax/metax_kernel_common.h"
#include "rms_norm_backward_metax.h"
#include "../info.h"

namespace op::rms_norm_backward::metax {

// Device-side type conversion functions
template<typename T>
__device__ float device_cast_to_float(T val) {
    return static_cast<float>(val);
}

template<>
__device__ float device_cast_to_float<fp16_t>(fp16_t val) {
    // Convert custom fp16_t to __half first, then to float
    __half h_val;
    memcpy(&h_val, &val, sizeof(__half));
    return __half2float(h_val);
}

template<>
__device__ float device_cast_to_float<bf16_t>(bf16_t val) {
    // Convert custom bf16_t to __hpcc_bfloat16 first, then to float
    __hpcc_bfloat16 bf_val;
    memcpy(&bf_val, &val, sizeof(__hpcc_bfloat16));
    return __bfloat162float(bf_val);
}

template<>
__device__ float device_cast_to_float<float>(float val) {
    return val;
}

template<typename T>
__device__ T device_cast_from_float(float val) {
    return static_cast<T>(val);
}

template<>
__device__ fp16_t device_cast_from_float<fp16_t>(float val) {
    // Convert float to __half first, then to custom fp16_t
    __half h_val = __float2half(val);
    fp16_t result;
    memcpy(&result, &h_val, sizeof(fp16_t));
    return result;
}

template<>
__device__ bf16_t device_cast_from_float<bf16_t>(float val) {
    // Convert float to __hpcc_bfloat16 first, then to custom bf16_t
    __hpcc_bfloat16 bf_val = __float2bfloat16(val);
    bf16_t result;
    memcpy(&result, &bf_val, sizeof(bf16_t));
    return result;
}

/**
 * @brief RMS Norm backward kernel for Metax platform
 * 
 * This kernel computes the backward pass for RMS normalization, calculating
 * gradients for both input (grad_x) and weight (grad_w) tensors.
 */
template <unsigned int BLOCK_SIZE, typename Tdata, typename Tweight = Tdata>
__global__ void rmsNormBackwardKernel(
    Tdata * grad_x,
    float * grad_w_cuda,
    const Tdata * grad_y,
    const Tdata * x,
    const Tweight * w,
    size_t ndim,
    size_t batch_size,
    size_t norm_size,
    const ptrdiff_t *__restrict__ grad_x_strides,
    const size_t *__restrict__ shape,
    const ptrdiff_t *__restrict__ grad_y_strides,
    const ptrdiff_t *__restrict__ x_strides,
    ptrdiff_t w_stride,
    float epsilon
) {
    // Calculate tensor pointers for current batch
    size_t batch_index = blockIdx.x;
    
    // Calculate multi-dimensional indices
    size_t batch_indices[8]; // Assume maximum 8 dimensions
    size_t remaining = batch_index;
    for (int dim = ndim - 2; dim >= 0; --dim) {
        batch_indices[dim] = remaining % shape[dim];
        remaining /= shape[dim];
    }
    
    // Calculate batch offsets for each tensor
    size_t x_batch_offset = 0;
    size_t grad_y_batch_offset = 0;
    size_t grad_x_batch_offset = 0;
    for (size_t dim = 0; dim < ndim - 1; ++dim) {
        x_batch_offset += batch_indices[dim] * x_strides[dim];
        grad_y_batch_offset += batch_indices[dim] * grad_y_strides[dim];
        grad_x_batch_offset += batch_indices[dim] * grad_x_strides[dim];
    }
    
    auto grad_w_ptr = grad_w_cuda + batch_index;

    // Calculate RMS following CPU algorithm - using mean_square instead of rsqrt
    float sum_squares = 0.0f;
    for (size_t i = threadIdx.x; i < norm_size; i += BLOCK_SIZE) {
        size_t x_idx = x_batch_offset + i * x_strides[ndim - 1];
        float x_val = device_cast_to_float(x[x_idx]);
        sum_squares += x_val * x_val;
    }
    
    // Reduce sum of squares across threads using manual block reduction
    __shared__ float shared_squares[BLOCK_SIZE];
    shared_squares[threadIdx.x] = sum_squares;
    __syncthreads();
    
    for (int stride = BLOCK_SIZE / 2; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shared_squares[threadIdx.x] += shared_squares[threadIdx.x + stride];
        }
        __syncthreads();
    }
    
    sum_squares = shared_squares[0];
    
    float norm_size_f = static_cast<float>(norm_size);

    // Shared memory for RMS and gradient accumulation
    __shared__ float rms;
    __shared__ float sum_grad_y_w_x;
    
    if (threadIdx.x == 0) {
        // Calculate RMS following CPU algorithm: sqrt(mean_square + epsilon)
        float mean_square = sum_squares / norm_size_f;
        rms = sqrtf(mean_square + epsilon);
        
        // Calculate sum(grad_y * w * x) following CPU algorithm
        sum_grad_y_w_x = 0.0f;
        for (size_t i = 0; i < norm_size; i++) {
            size_t x_idx = x_batch_offset + i * x_strides[ndim - 1];
            size_t grad_y_idx = grad_y_batch_offset + i * grad_y_strides[ndim - 1];
            size_t w_idx = i * w_stride;
            
            float gy = device_cast_to_float(grad_y[grad_y_idx]);
            float w_val = device_cast_to_float(w[w_idx]);
            float x_val = device_cast_to_float(x[x_idx]);
            sum_grad_y_w_x += gy * w_val * x_val;
        }
    }
    __syncthreads();

    float rms_val = rms;
    float rms_squared = rms_val * rms_val;
    float rms_cubed = rms_squared * rms_val;
    float sum_val = sum_grad_y_w_x;

    // Calculate gradients following CPU algorithm
    for (size_t i = threadIdx.x; i < norm_size; i += BLOCK_SIZE) {
        size_t x_idx = x_batch_offset + i * x_strides[ndim - 1];
        size_t grad_y_idx = grad_y_batch_offset + i * grad_y_strides[ndim - 1];
        size_t grad_x_idx = grad_x_batch_offset + i * grad_x_strides[ndim - 1];
        size_t w_idx = i * w_stride;
        
        float gy = device_cast_to_float(grad_y[grad_y_idx]);
        float w_val = device_cast_to_float(w[w_idx]);
        float x_val = device_cast_to_float(x[x_idx]);
        
        // RMSNorm grad_x calculation: (w * grad_y) / rms - (x * sum(grad_y * w * x)) / (norm_size * rmsÂ³)
        float gx = (w_val * gy) / rms_val - (x_val * sum_val) / (norm_size_f * rms_cubed);
        grad_x[grad_x_idx] = device_cast_from_float<Tdata>(gx);
        
        // grad_w calculation: (x * grad_y) / rms
        float gw = (x_val * gy) / rms_val;
        *(grad_w_ptr + i * batch_size) = gw;
    }
}

/**
 * @brief Kernel to sum up weight gradients across batches
 */
template <unsigned int BLOCK_SIZE, typename Tdata>
__global__ void sumUpGradWKernel(
    Tdata * grad_w,
    float * grad_w_cuda,
    size_t batch_size,
    ptrdiff_t grad_w_stride
) {
    size_t norm_index = blockIdx.x;
    
    // Reduce weight gradients across all batches for this feature dimension
    float sum_grad_w = 0.0f;
    for (int i = threadIdx.x; i < batch_size; i += blockDim.x) {
        sum_grad_w += grad_w_cuda[norm_index * batch_size + i];
    }
    
    // Block-level reduction
    __shared__ float shared_sum[BLOCK_SIZE];
    shared_sum[threadIdx.x] = sum_grad_w;
    __syncthreads();
    
    for (int stride = BLOCK_SIZE / 2; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shared_sum[threadIdx.x] += shared_sum[threadIdx.x + stride];
        }
        __syncthreads();
    }
    
    sum_grad_w = shared_sum[0];
    
    // Write the final gradient value
    if (threadIdx.x == 0) {
        *(grad_w + norm_index * grad_w_stride) = device_cast_from_float<Tdata>(sum_grad_w);
    }
}

/**
 * @brief Launch wrapper for RMS Norm backward kernel
 */
template <unsigned int BLOCK_SIZE, typename Tdata, typename Tweight>
static infiniStatus_t launchRmsNormBackwardKernel(
    const RMSNormBackwardInfo &info,
    Tdata * grad_x,
    float * grad_w_cuda,
    const Tdata * grad_y,
    const Tdata * x,
    const Tweight * w,
    hcStream_t stream,
    void * workspace
) {
    size_t ndim = info.ndim();
    
    // Prepare stride and shape arrays in device memory
    size_t * shape_cuda = reinterpret_cast<size_t*>(workspace);
    ptrdiff_t * grad_x_strides_cuda = reinterpret_cast<ptrdiff_t*>(shape_cuda + ndim);
    ptrdiff_t * grad_y_strides_cuda = grad_x_strides_cuda + ndim;
    ptrdiff_t * x_strides_cuda = grad_y_strides_cuda + ndim;

    // Copy stride and shape data to device
    CHECK_METAX(hcMemcpyAsync(shape_cuda, info.shape.data(), 
                               sizeof(size_t) * ndim, hcMemcpyHostToDevice, stream));
    CHECK_METAX(hcMemcpyAsync(grad_x_strides_cuda, info.grad_x_strides.data(), 
                               sizeof(ptrdiff_t) * ndim, hcMemcpyHostToDevice, stream));
    CHECK_METAX(hcMemcpyAsync(grad_y_strides_cuda, info.grad_y_strides.data(), 
                               sizeof(ptrdiff_t) * ndim, hcMemcpyHostToDevice, stream));
    CHECK_METAX(hcMemcpyAsync(x_strides_cuda, info.x_strides.data(), 
                               sizeof(ptrdiff_t) * ndim, hcMemcpyHostToDevice, stream));

    // Launch the main backward kernel
    rmsNormBackwardKernel<BLOCK_SIZE, Tdata, Tweight><<<info.batch_size(), BLOCK_SIZE, 0, stream>>>(
        grad_x, grad_w_cuda, grad_y, x, w,
        ndim, info.batch_size(), info.dim(),
        grad_x_strides_cuda, shape_cuda,
        grad_y_strides_cuda, x_strides_cuda,
        info.w_strides[0],
        info.epsilon
    );
    
    CHECK_METAX(hcGetLastError());
    return INFINI_STATUS_SUCCESS;
}

/**
 * @brief Launch wrapper for weight gradient summation kernel
 */
template <unsigned int BLOCK_SIZE, typename Tdata>
static infiniStatus_t launchSumUpGradWKernel(
    const RMSNormBackwardInfo &info,
    Tdata * grad_w,
    float * grad_w_cuda,
    hcStream_t stream
) {
    sumUpGradWKernel<BLOCK_SIZE, Tdata><<<info.dim(), BLOCK_SIZE, 0, stream>>>(
        grad_w, grad_w_cuda, info.batch_size(), info.grad_w_strides[0]
    );
    
    CHECK_METAX(hcGetLastError());
    return INFINI_STATUS_SUCCESS;
}

/**
 * @brief Main computation function for RMS Norm backward pass
 */
template<unsigned int BLOCK_SIZE, typename Tdata, typename Tweight>
static infiniStatus_t calculate_rms_norm_backward(
    const RMSNormBackwardInfo &info,
    Tdata * grad_x,
    Tdata * grad_w,
    const Tdata * grad_y,
    const Tdata * x,
    const Tweight * w,
    hcStream_t stream,
    void * workspace
) {
    // Calculate workspace layout
    size_t ndim = info.ndim();
    size_t stride_workspace_size = sizeof(ptrdiff_t) * ndim * 4;
    float * grad_w_cuda = reinterpret_cast<float *>(
        reinterpret_cast<char*>(workspace) + stride_workspace_size
    );

    // Stage 1: Compute input gradients and intermediate weight gradients
    auto status1 = launchRmsNormBackwardKernel<BLOCK_SIZE, Tdata, Tweight>(
        info, grad_x, grad_w_cuda, grad_y, x, w, stream, workspace
    );
    CHECK_STATUS(status1);

    // Stage 2: Sum up weight gradients across batches
    auto status2 = launchSumUpGradWKernel<BLOCK_SIZE, Tdata>(
        info, grad_w, grad_w_cuda, stream
    );
    CHECK_STATUS(status2);
    
    return INFINI_STATUS_SUCCESS;
}

// Template instantiation for different data types
template<unsigned int BLOCK_SIZE, typename Tdata, typename Tweight>
infiniStatus_t rmsNormBackwardMetax(
    const RMSNormBackwardInfo &info,
    Tdata * grad_x,
    Tdata * grad_w,
    const Tdata * grad_y,
    const Tdata * x,
    const Tweight * w,
    void * stream,
    void * workspace
) {
    auto cuda_stream = reinterpret_cast<hcStream_t>(stream);
    return calculate_rms_norm_backward<BLOCK_SIZE, Tdata, Tweight>(
        info, grad_x, grad_w, grad_y, x, w, cuda_stream, workspace
    );
}

// Descriptor implementation
struct Descriptor::Opaque {
    std::shared_ptr<device::metax::Handle::Internal> internal;
};

Descriptor::~Descriptor() {
    delete _opaque;
}

infiniStatus_t Descriptor::create(
    infiniopHandle_t handle_,
    Descriptor **desc_ptr,
    infiniopTensorDescriptor_t grad_x_desc,
    infiniopTensorDescriptor_t grad_w_desc,
    infiniopTensorDescriptor_t grad_y_desc,
    infiniopTensorDescriptor_t x_desc,
    infiniopTensorDescriptor_t w_desc,
    float epsilon) {
    
    auto info_result = RMSNormBackwardInfo::createRMSNormBackwardInfo(
        grad_x_desc, grad_w_desc, grad_y_desc, x_desc, w_desc, epsilon
    );
    if (!info_result) {
        return info_result.status();
    }
    RMSNormBackwardInfo info = info_result.take();

    auto handle = reinterpret_cast<device::metax::Handle *>(handle_);
    size_t workspace_size = sizeof(ptrdiff_t) * info.ndim() * 4 + sizeof(float) * info.batch_size() * info.dim();

    *desc_ptr = new Descriptor(
        new Opaque{handle->internal()},
        std::move(info),
        workspace_size,
        handle_->device, handle_->device_id);
    return INFINI_STATUS_SUCCESS;
}

infiniStatus_t Descriptor::calculate(
    void *workspace, size_t workspace_size,
    void *grad_x,
    void *grad_w,
    const void *grad_y,
    const void *x,
    const void *w,
    void *stream) const {
    
    if (workspace_size < _workspace_size) {
        return INFINI_STATUS_INSUFFICIENT_WORKSPACE;
    }

    // Use the two-stage architecture with proper data type handling
    if (_info.grad_x_dtype == INFINI_DTYPE_F16 && _info.w_dtype == INFINI_DTYPE_F16) {
        return rmsNormBackwardMetax<1024, fp16_t, fp16_t>(
            _info, 
            reinterpret_cast<fp16_t*>(grad_x),
            reinterpret_cast<fp16_t*>(grad_w),
            reinterpret_cast<const fp16_t*>(grad_y),
            reinterpret_cast<const fp16_t*>(x),
            reinterpret_cast<const fp16_t*>(w),
            stream,
            workspace
        );
    } else if (_info.grad_x_dtype == INFINI_DTYPE_F16 && _info.w_dtype == INFINI_DTYPE_F32) {
        return rmsNormBackwardMetax<1024, fp16_t, float>(
            _info,
            reinterpret_cast<fp16_t*>(grad_x),
            reinterpret_cast<fp16_t*>(grad_w),
            reinterpret_cast<const fp16_t*>(grad_y),
            reinterpret_cast<const fp16_t*>(x),
            reinterpret_cast<const float*>(w),
            stream,
            workspace
        );
    } else if (_info.grad_x_dtype == INFINI_DTYPE_BF16 && _info.w_dtype == INFINI_DTYPE_BF16) {
        return rmsNormBackwardMetax<1024, bf16_t, bf16_t>(
            _info,
            reinterpret_cast<bf16_t*>(grad_x),
            reinterpret_cast<bf16_t*>(grad_w),
            reinterpret_cast<const bf16_t*>(grad_y),
            reinterpret_cast<const bf16_t*>(x),
            reinterpret_cast<const bf16_t*>(w),
            stream,
            workspace
        );
    } else if (_info.grad_x_dtype == INFINI_DTYPE_BF16 && _info.w_dtype == INFINI_DTYPE_F32) {
        return rmsNormBackwardMetax<1024, bf16_t, float>(
            _info,
            reinterpret_cast<bf16_t*>(grad_x),
            reinterpret_cast<bf16_t*>(grad_w),
            reinterpret_cast<const bf16_t*>(grad_y),
            reinterpret_cast<const bf16_t*>(x),
            reinterpret_cast<const float*>(w),
            stream,
            workspace
        );
    } else if (_info.grad_x_dtype == INFINI_DTYPE_F32 && _info.w_dtype == INFINI_DTYPE_F32) {
        return rmsNormBackwardMetax<1024, float, float>(
            _info,
            reinterpret_cast<float*>(grad_x),
            reinterpret_cast<float*>(grad_w),
            reinterpret_cast<const float*>(grad_y),
            reinterpret_cast<const float*>(x),
            reinterpret_cast<const float*>(w),
            stream,
            workspace
        );
    } else {
        return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }
}

} // namespace op::rms_norm_backward::metax