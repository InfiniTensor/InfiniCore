#include "layer_norm_backward_metax.h"
#include "layer_norm_backward_kernel.cuh"
#include "../../../devices/metax/metax_common.h"
#include "../../../devices/metax/metax_handle.h"
#include "../../../tensor.h"
#include "../../../../utils/custom_types.h"
#include <hcr/hc_runtime_api.h>
#include <cmath>

// Error checking macro for MetaX
#define CHECK_METAX(call) do { \
    hcError_t err = call; \
    if (err != hcSuccess) { \
        return INFINI_STATUS_EXECUTION_FAILED; \
    } \
} while(0)

namespace op::layer_norm_backward::metax {

// Forward declarations
template <typename GradInputType, typename AccType, typename WeightType>
infiniStatus_t launchLayerNormBackwardKernel(
    void *grad_input_data,
    void *grad_weight_data,
    void *grad_bias_data,
    const void *grad_output_data,
    const void *input_data,
    const void *weight_data,
    const void *input_std_deviation_data,
    const void *input_standardization_data,
    const LayerNormBackwardInfo &info,
    void *stream);

template <typename AccType, typename WeightType>
infiniStatus_t launchSumUpGradWKernel(
    void *grad_weight_data,
    void *grad_bias_data,
    const void *workspace,
    const LayerNormBackwardInfo &info,
    void *stream);

Descriptor::~Descriptor() {}

infiniStatus_t Descriptor::create(
    infiniopHandle_t handle_,
    Descriptor **desc_ptr,
    infiniopTensorDescriptor_t grad_input_desc,
    infiniopTensorDescriptor_t grad_weight_desc,
    infiniopTensorDescriptor_t grad_bias_desc,
    infiniopTensorDescriptor_t grad_output_desc,
    infiniopTensorDescriptor_t input_desc,
    infiniopTensorDescriptor_t weight_desc,
    infiniopTensorDescriptor_t input_std_deviation_desc,
    infiniopTensorDescriptor_t input_standardization_desc,
    float epsilon) {
    
    if (!handle_ || !desc_ptr) {
        return INFINI_STATUS_BAD_PARAM;
    }
    
    auto handle = static_cast<device::metax::Handle *>(handle_);
    
    // 创建 LayerNormBackwardInfo
    auto info_result = LayerNormBackwardInfo::create(
        grad_input_desc, grad_weight_desc, grad_bias_desc, grad_output_desc,
        input_desc, weight_desc, input_std_deviation_desc, input_standardization_desc,
        epsilon);
    
    if (!info_result) {
        return info_result.status();
    }
    
    auto info = info_result.take();
    
    // 计算工作区大小
    size_t workspace_size = 0;
    if (info.has_bias) {
        workspace_size += info._batch_size * info._normalized_size * sizeof(float);
    }
    workspace_size += info._batch_size * info._normalized_size * sizeof(float);
    
    // 创建描述符
    *desc_ptr = new Descriptor(info, workspace_size, handle->device, handle->device_id);
    
    return INFINI_STATUS_SUCCESS;
}

infiniStatus_t Descriptor::calculate(
    void *workspace,
    size_t workspace_size,
    void *grad_input,
    void *grad_weight,
    void *grad_bias,
    const void *grad_output,
    const void *input,
    const void *weight,
    const void *input_std_deviation,
    const void *input_standardization,
    void *stream) const {
    
    if (workspace_size < _workspace_size) {
        return INFINI_STATUS_INSUFFICIENT_WORKSPACE;
    }
    
    // Dispatch based on data types
    switch (_info.dtype) {
        case INFINI_DTYPE_F16:
            return calculate_layer_norm_backward<half, float, half>(
                workspace, grad_input, grad_weight, grad_bias, grad_output,
                input, weight, input_std_deviation, input_standardization, _info, stream);
        case INFINI_DTYPE_F32:
            return calculate_layer_norm_backward<float, float, float>(
                workspace, grad_input, grad_weight, grad_bias, grad_output,
                input, weight, input_std_deviation, input_standardization, _info, stream);
        default:
            return INFINI_STATUS_BAD_PARAM;
    }
}

template <typename GradInputType, typename AccType, typename WeightType>
infiniStatus_t Descriptor::calculate_layer_norm_backward(
    void *workspace,
    void *grad_input_data,
    void *grad_weight_data,
    void *grad_bias_data,
    const void *grad_output_data,
    const void *input_data,
    const void *weight_data,
    const void *input_std_deviation_data,
    const void *input_standardization_data,
    const LayerNormBackwardInfo &info,
    void *stream) const {
    
    // Launch layer norm backward kernel
    auto status = launchLayerNormBackwardKernel<GradInputType, AccType, WeightType>(
        grad_input_data, grad_weight_data, grad_bias_data, grad_output_data,
        input_data, weight_data, input_std_deviation_data, input_standardization_data,
        info, stream);
    
    if (status != INFINI_STATUS_SUCCESS) {
        return status;
    }
    
    // Launch sum up gradients kernel if needed
    if (grad_weight_data || grad_bias_data) {
        status = launchSumUpGradWKernel<AccType, WeightType>(
            grad_weight_data, grad_bias_data, workspace, info, stream);
        
        if (status != INFINI_STATUS_SUCCESS) {
            return status;
        }
    }
    
    return INFINI_STATUS_SUCCESS;
}

// Kernel implementations
template <typename GradInputType, typename AccType, typename WeightType>
infiniStatus_t launchLayerNormBackwardKernel(
    void *grad_input_data,
    void *grad_weight_data,
    void *grad_bias_data,
    const void *grad_output_data,
    const void *input_data,
    const void *weight_data,
    const void *input_std_deviation_data,
    const void *input_standardization_data,
    const LayerNormBackwardInfo &info,
    void *stream) {
    
    // TODO: Implement actual MetaX kernel launch
    // This is a placeholder implementation
    return INFINI_STATUS_SUCCESS;
}

template <typename AccType, typename WeightType>
infiniStatus_t launchSumUpGradWKernel(
    void *grad_weight_data,
    void *grad_bias_data,
    const void *workspace,
    const LayerNormBackwardInfo &info,
    void *stream) {
    
    // TODO: Implement actual MetaX kernel launch
    // This is a placeholder implementation
    return INFINI_STATUS_SUCCESS;
}

// Type aliases for MetaX
using half = __half;
using cuda_bfloat16 = hpcc_bfloat16;

// Explicit template instantiations
template infiniStatus_t launchLayerNormBackwardKernel<half, float, half>(
    void *, void *, void *, const void *, const void *, const void *,
    const void *, const void *, const LayerNormBackwardInfo &, void *);

template infiniStatus_t launchLayerNormBackwardKernel<float, float, float>(
    void *, void *, void *, const void *, const void *, const void *,
    const void *, const void *, const LayerNormBackwardInfo &, void *);

template infiniStatus_t launchSumUpGradWKernel<float, half>(
    void *, void *, const void *, const LayerNormBackwardInfo &, void *);

template infiniStatus_t launchSumUpGradWKernel<float, float>(
    void *, void *, const void *, const LayerNormBackwardInfo &, void *);

} // namespace op::layer_norm_backward::metax