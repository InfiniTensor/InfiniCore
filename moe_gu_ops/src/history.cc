GuMoeExperts::GuMoeExperts(int num_experts, int hidden_dim, int intermediate_dim, const DataType& dtype, const Device& device)

: num_experts_(num_experts), hidden_dim_(hidden_dim), intermediate_dim_(intermediate_dim), device_(device) {

infinirtSetDevice((infiniDevice_t)device.getType(), device.getIndex());

infiniopCreateHandle(&this->handle_);

INFINICORE_NN_PARAMETER_INIT(gate_up_proj, ({ {static_cast<size_t>(num_experts), static_cast<size_t>(2 * intermediate_dim), static_cast<size_t>(hidden_dim)}, dtype, device }));

INFINICORE_NN_PARAMETER_INIT(down_proj, ({ {static_cast<size_t>(num_experts), static_cast<size_t>(hidden_dim), static_cast<size_t>(intermediate_dim)}, dtype, device }));

}

GuMoeExperts::~GuMoeExperts() { if (handle_) infiniopDestroyHandle(handle_); }



Tensor GuMoeExperts::forward(const Tensor& hidden_states, const Tensor& top_k_index, const Tensor& top_k_values) const {

if (hidden_states->dtype() != DataType::F32) throw std::runtime_error("F32 only");



Device gpu = hidden_states->device();

Device cpu(Device::Type::CPU);



Tensor cpu_indices = top_k_index->to(cpu);

Tensor cpu_values = top_k_values->to(cpu);

Tensor cpu_hidden = hidden_states->to(cpu);


Tensor final_cpu_states = Tensor::zeros(hidden_states->shape(), hidden_states->dtype(), cpu);

std::memset(final_cpu_states->data(), 0, final_cpu_states->numel() * sizeof(float));



size_t total_tokens = hidden_states->numel() / hidden_dim_;

int top_k = top_k_index->shape()[1];



struct Task { int token_idx; int rank_idx; };

std::vector<std::vector<Task>> buckets(num_experts_);

const void* raw_idx = cpu_indices->data();


bool is_i32 = (cpu_indices->dtype() == DataType::I32);

const float* all_vals = (const float*)cpu_values->data();



// è·¯ç”±ä¿¡æ¯æ‰“å° (ä¿æŒä½ ä¹‹å‰çš„)

static bool debug_printed = false;

if (!debug_printed) {

std::cout << "\n[C++ Debug Info]" << std::endl;

std::cout << "Token 0 Selected Experts: [";

for(int k=0; k<top_k; ++k) {

int64_t val = is_i32 ? (int64_t)((const int32_t*)raw_idx)[k] : ((const int64_t*)raw_idx)[k];

std::cout << val << (k == top_k - 1 ? "" : ", ");

}

std::cout << "]" << std::endl;

std::cout << "Token 0 Expert Weights: [";

for(int k=0; k<top_k; ++k) {

std::cout << all_vals[k] << (k == top_k - 1 ? "" : ", ");

}

std::cout << "]" << std::endl;

}



for (size_t i = 0; i < total_tokens; ++i) {

for (size_t k = 0; k < static_cast<size_t>(top_k); ++k) {

int64_t val = is_i32 ? (int64_t)((const int32_t*)raw_idx)[i*top_k+k] : ((const int64_t*)raw_idx)[i*top_k+k];

int eid = (int)val;

if (eid >= 0 && eid < num_experts_) buckets[eid].push_back({(int)i, (int)k});

}

}



infinirtSetDevice((infiniDevice_t)device_.getType(), device_.getIndex());



for (int e = 0; e < num_experts_; ++e) {

if (buckets[e].empty()) continue;

size_t n = buckets[e].size();



std::vector<int> t_idx(n);

std::vector<float> t_w(n);


// æŸ¥æ‰¾ Token 0 åœ¨å½“å‰ bucket ä¸­çš„ä½ç½®

int local_token0_idx = -1;



for(size_t i=0; i<n; ++i) {

t_idx[i] = buckets[e][i].token_idx;

if (t_idx[i] == 0) local_token0_idx = (int)i; // æ ‡è®°ä½ç½®

t_w[i] = all_vals[t_idx[i]*top_k + buckets[e][i].rank_idx];

}



Tensor cpu_in = Tensor::empty({n, (size_t)hidden_dim_}, hidden_states->dtype(), cpu);

cpu_gather((float*)cpu_in->data(), (const float*)cpu_hidden->data(), t_idx, hidden_dim_);

Tensor gpu_in = cpu_in->to(gpu);



Tensor w_gate_up = gate_up_proj_->narrow({{0, (size_t)e, 1}})->view({(size_t)(2*intermediate_dim_), (size_t)hidden_dim_});

Tensor gate_up_out = infinicore::op::linear(gpu_in, w_gate_up, std::nullopt);



// =====================================================================

// ğŸ•µï¸â€â™‚ï¸ [æ–°å¢] FFN ä¸­é—´å€¼æ¢é’ˆ (Gate/Up)

// =====================================================================

static bool ffn_debug_printed = false;

if (!ffn_debug_printed && local_token0_idx != -1) {

std::cout << "\n[C++ FFN Internal Debug] Expert " << e << " processing Token 0" << std::endl;


// æ‹·å› CPU

Tensor debug_tensor = gate_up_out->to(cpu);

const float* ptr = (const float*)debug_tensor->data();


// å®šä½åˆ° Token 0 çš„é‚£ä¸€è¡Œæ•°æ®

// shape: [n, 2 * intermediate_dim]

size_t row_offset = local_token0_idx * (2 * intermediate_dim_);

const float* token0_row = ptr + row_offset;



// æ‰“å°å‰åŠéƒ¨åˆ† (C++ è®¤ä¸ºæ˜¯ Gate)

std::cout << " C++ First Half (Gate?): [";

for(int j=0; j<5; ++j) std::cout << token0_row[j] << ", ";

std::cout << "...]" << std::endl;



// æ‰“å°ååŠéƒ¨åˆ† (C++ è®¤ä¸ºæ˜¯ Up)

size_t mid = intermediate_dim_;

std::cout << " C++ Second Half (Up?): [";

for(int j=0; j<5; ++j) std::cout << token0_row[mid+j] << ", ";

std::cout << "...]" << std::endl;


ffn_debug_printed = true;

}

// =====================================================================



Tensor gate = gate_up_out->narrow({{1, 0, (size_t)intermediate_dim_}});

Tensor up = gate_up_out->narrow({{1, (size_t)intermediate_dim_, (size_t)intermediate_dim_}});

Tensor ffn_inner = infinicore::op::mul(infinicore::op::silu(gate), up, this->handle_);


Tensor w_down = down_proj_->narrow({{0, (size_t)e, 1}})->view({(size_t)hidden_dim_, (size_t)intermediate_dim_});

Tensor gpu_res = infinicore::op::linear(ffn_inner, w_down, std::nullopt);


Tensor cpu_res = gpu_res->to(cpu);



infinirtDeviceSynchronize();



cpu_index_add_scale((float*)final_cpu_states->data(),

(const float*)cpu_res->data(),

t_idx, t_w, hidden_dim_,

total_tokens);

}


if (!debug_printed) debug_printed = true; // é˜²æ­¢æ¼æ‰“å¯¼è‡´å¤šæ¬¡



return final_cpu_states->to(gpu);

}

// #include "gu_moe.h" 

// #include <cstring>
// #include <stdexcept>
// #include <vector>
// #include <cstdint> 
// #include <iostream> 
// #include <iomanip>  
// #include <cmath>    

// #include "src/nvidia_kernels/nvidia_kernels_moe.h"
// #include "infinicore/ops.hpp"
// #include "infinicore/ops/linear.hpp"
// #include "infinirt.h" 
// #include "infiniop.h" 
// #include "gu_mul.h"
// #include "gu_topk_softmax.h"

// namespace infinicore::nn {

// namespace {

// void debug_tensor(const std::string& name, const Tensor& t, int count=5) {
//     Device cpu(Device::Type::CPU);
//     Tensor c = t->to(cpu);
//     if (c->dtype() == DataType::F32) {
//         const float* ptr = reinterpret_cast<const float*>(c->data());
//         float min_v = 1e30, max_v = -1e30;
//         double sum = 0;
//         for(size_t i=0; i<c->numel(); ++i) {
//             float v = ptr[i];
//             if(v < min_v) min_v = v;
//             if(v > max_v) max_v = v;
//             sum += std::abs(v);
//         }
//         std::cout << "[DEBUG] " << name << " | Min: " << min_v << " | Max: " << max_v 
//                   << " | MeanAbs: " << (sum / c->numel()) << std::endl;
//     }
// }

// void cpu_gather(float* dest, const float* src, const std::vector<int>& indices, int hidden_dim) {
//     for (size_t i = 0; i < indices.size(); ++i) {
//         int row = indices[i];
//         std::memcpy(dest + i * hidden_dim, src + row * hidden_dim, hidden_dim * sizeof(float));
//     }
// }

// void cpu_index_add_scale(float* dest, const float* src, 
//                          const std::vector<int>& indices, 
//                          const std::vector<float>& weights, 
//                          int hidden_dim, 
//                          size_t total_rows) { 
//     for (size_t i = 0; i < indices.size(); ++i) {
//         int row = indices[i];
//         if (row < 0 || row >= (int)total_rows) continue; 
//         float w = weights[i];
//         float* d_row = dest + row * hidden_dim;
//         const float* s_row = src + i * hidden_dim;
//         for (int j = 0; j < hidden_dim; ++j) {
//             d_row[j] += s_row[j] * w;
//         }
//     }
// }

// } // namespace anonymous

// // ... GuMoeTopkRounter ...
// GuMoeTopkRounter::GuMoeTopkRounter(int num_experts, int hidden_dim, int top_k, bool norm_topk_prob, const DataType &dtype, const Device &device)
//     : top_k_(top_k), num_experts_(num_experts), hidden_dim_(hidden_dim), norm_topk_prob_(norm_topk_prob) {
//     infinirtSetDevice((infiniDevice_t)device.getType(), device.getIndex());
//     infiniopCreateHandle(&this->handle_);
//     INFINICORE_NN_PARAMETER_INIT(weight, ({ {static_cast<size_t>(num_experts_), static_cast<size_t>(hidden_dim_)}, dtype, device }));
// }
// GuMoeTopkRounter::~GuMoeTopkRounter() { if (handle_) infiniopDestroyHandle(handle_); }

// std::pair<Tensor, Tensor> GuMoeTopkRounter::forward(const Tensor &hidden_states) const {
//     size_t total_tokens = hidden_states->numel() / hidden_dim_;
//     Tensor flattened = hidden_states->view({total_tokens, static_cast<size_t>(hidden_dim_)});
//     Tensor logits = infinicore::op::linear(flattened, weight_, std::nullopt);
//     auto [val, idx] = infinicore::op::topk_softmax(logits, top_k_, norm_topk_prob_, this->handle_);
//     return {val, idx};
// }

// // ... GuMoeExperts ...
// GuMoeExperts::GuMoeExperts(int num_experts, int hidden_dim, int intermediate_dim, const DataType& dtype, const Device& device)
//     : num_experts_(num_experts), hidden_dim_(hidden_dim), intermediate_dim_(intermediate_dim), device_(device) {
//     infinirtSetDevice((infiniDevice_t)device.getType(), device.getIndex());
//     infiniopCreateHandle(&this->handle_);
//     INFINICORE_NN_PARAMETER_INIT(gate_up_proj, ({ {static_cast<size_t>(num_experts), static_cast<size_t>(2 * intermediate_dim), static_cast<size_t>(hidden_dim)}, dtype, device }));
//     INFINICORE_NN_PARAMETER_INIT(down_proj, ({ {static_cast<size_t>(num_experts), static_cast<size_t>(hidden_dim), static_cast<size_t>(intermediate_dim)}, dtype, device }));
// }
// GuMoeExperts::~GuMoeExperts() { if (handle_) infiniopDestroyHandle(handle_); }

// Tensor GuMoeExperts::forward(const Tensor& hidden_states, const Tensor& top_k_index, const Tensor& top_k_values) const {
//     if (hidden_states->dtype() != DataType::F32) throw std::runtime_error("F32 only");

//     // 0. ä¸Šä¸‹æ–‡å‡†å¤‡
//     Device device = hidden_states->device();
//     // å‡è®¾ä½¿ç”¨é»˜è®¤æµ 0ã€‚å¦‚æœ infiniop æ”¯æŒè·å–æµï¼Œå»ºè®®ä½¿ç”¨ context::getStream()
//     cudaStream_t stream = 0;

//     size_t num_tokens = hidden_states->numel() / hidden_dim_;
//     int top_k = top_k_index->shape()[1];
//     size_t expanded_size = num_tokens * top_k;

//     // 1. åˆ†é… GPU æ˜¾å­˜ (Workspace)
//     // å·¥ä¸šçº§ä¼˜åŒ–ç‚¹ï¼šè¿™é‡Œçš„ Tensor::zeros/empty æ¯æ¬¡ forward éƒ½ä¼šç”³è¯·æ˜¾å­˜ã€‚
//     // å¦‚æœè¿½æ±‚æè‡´æ€§èƒ½ï¼Œå»ºè®®åœ¨ç±»é‡Œç»´æŠ¤ä¸€ä¸ªç¼“å­˜æ±  (Tensor workspace_)ã€‚
    
//     // è®¡æ•°å™¨å’Œåç§»é‡
//     Tensor expert_counts = Tensor::zeros({(size_t)num_experts_}, DataType::I32, device);
//     Tensor expert_offsets = Tensor::zeros({(size_t)num_experts_ + 1}, DataType::I32, device);

//     // ä¸­é—´ buffer (æ’åºåçš„è¾“å…¥/è¾“å‡º)
//     Tensor sorted_input = Tensor::empty({expanded_size, (size_t)hidden_dim_}, DataType::F32, device);
//     Tensor sorted_output = Tensor::empty({expanded_size, (size_t)hidden_dim_}, DataType::F32, device);
    
//     // è¾…åŠ©ä¿¡æ¯ (Row Map å’Œ Weights)
//     Tensor sorted_row_map = Tensor::empty({expanded_size}, DataType::I32, device);
//     Tensor sorted_weights = Tensor::empty({expanded_size}, DataType::F32, device);
    
//     // æœ€ç»ˆè¾“å‡º (å¿…é¡»åˆå§‹åŒ–ä¸º 0ï¼Œå› ä¸º Reduce æ˜¯ç´¯åŠ )
//     Tensor final_output = Tensor::zeros(hidden_states->shape(), DataType::F32, device);

//     // è·å–è£¸æŒ‡é’ˆ
//     float* d_input     = (float*)hidden_states->data();
//     int32_t* d_indices = (int32_t*)top_k_index->data();
//     float* d_values    = (float*)top_k_values->data();
    
//     int32_t* d_counts  = (int32_t*)expert_counts->data();
//     int32_t* d_offsets = (int32_t*)expert_offsets->data();

//     // ======================================================================
//     // Phase 1: æ•°æ®é‡æ’ (GPU Sort & Permute)
//     // å½»åº•å–ä»£åŸæ¥çš„ CPU bucket å’Œ cpu_gather
//     // ======================================================================
    
//     // 1.1 æ’åºï¼šè®¡ç®—æ¯ä¸ªä¸“å®¶çš„ Token æ•°é‡å’Œåç§»é‡
//     launch_moe_sort(
//         d_indices, d_counts, d_offsets, 
//         num_tokens, top_k, num_experts_, 
//         stream
//     );

//     // 1.2 æ¬è¿ï¼šå°† Input å’Œ Weights æŒ‰ç…§ä¸“å®¶é¡ºåºè¿ç»­æ’åˆ—åˆ° sorted_input/sorted_weights
//     // æ³¨æ„ï¼šå¤ç”¨ expert_counts ä½œä¸º running_counters (å†…éƒ¨ä¼šè‡ªåŠ¨æ¸…é›¶)
//     launch_moe_permute(
//         d_input, 
//         d_indices, 
//         d_values, 
//         d_offsets,
//         (float*)sorted_input->data(), 
//         (int32_t*)sorted_row_map->data(),
//         (float*)sorted_weights->data(),
//         d_counts, 
//         num_tokens, top_k, hidden_dim_, num_experts_, 
//         stream
//     );

//     // ======================================================================
//     // Phase 2: è®¡ç®— (GPU Loop)
//     // è¿™é‡Œçš„å¾ªç¯ä»…ç”¨äºå‘å°„ Kernelï¼Œæ•°æ®å…¨ç¨‹åœ¨ GPU ä¸Šï¼Œæ²¡æœ‰æ‹·è´å¼€é”€
//     // ======================================================================

//     // å°† Offsets æ‹·å› CPUï¼Œä»¥ä¾¿ CPU çŸ¥é“å¦‚ä½•å¯¹ sorted_input è¿›è¡Œåˆ‡ç‰‡
//     std::vector<int32_t> h_offsets(num_experts_ + 1);
//     cudaMemcpyAsync(h_offsets.data(), d_offsets, sizeof(int32_t) * (num_experts_ + 1), cudaMemcpyDeviceToHost, stream);
//     cudaStreamSynchronize(stream); // ç­‰å¾… Offset æ‹·è´å®Œæˆ

//     for (int e = 0; e < num_experts_; ++e) {
//         int start_idx = h_offsets[e];
//         int count = h_offsets[e+1] - start_idx;

//         // å¦‚æœè¯¥ä¸“å®¶æ²¡æœ‰åˆ†é…åˆ° Tokenï¼Œè·³è¿‡
//         if (count == 0) continue;

//         // A. åˆ‡ç‰‡ (Slicing - Zero Copy)
//         // è¿™é‡Œçš„ narrow åªæ˜¯åˆ›å»º Viewï¼Œä¸å‘ç”Ÿæ•°æ®æ¬è¿
//         // åˆ‡å‡ºå±äºå½“å‰ä¸“å®¶çš„è¾“å…¥æ•°æ®
//         Tensor expert_in = sorted_input->narrow({{0, (size_t)start_idx, (size_t)count}});

//         // åˆ‡å‡ºå½“å‰ä¸“å®¶çš„æƒé‡
//         Tensor w_gate_up = gate_up_proj_->narrow({{0, (size_t)e, 1}})->view({(size_t)(2*intermediate_dim_), (size_t)hidden_dim_});
//         Tensor w_down = down_proj_->narrow({{0, (size_t)e, 1}})->view({(size_t)hidden_dim_, (size_t)intermediate_dim_});

//         // B. è®¡ç®— (Computation - All on GPU)
//         // 1. Linear: Input * GateUp
//         Tensor gate_up_out = infinicore::op::linear(expert_in, w_gate_up, std::nullopt);

//         // 2. Activation: SiLU(Gate) * Up
//         Tensor gate = gate_up_out->narrow({{1, 0, (size_t)intermediate_dim_}});
//         Tensor up = gate_up_out->narrow({{1, (size_t)intermediate_dim_, (size_t)intermediate_dim_}});
        
//         // FFN Inner
//         Tensor ffn_inner = infinicore::op::mul(infinicore::op::silu(gate), up, this->handle_);

//         // 3. Linear: Inner * Down
//         Tensor expert_res = infinicore::op::linear(ffn_inner, w_down, std::nullopt);

//         // C. å†™å›å¤§ Buffer (Scatter back to sorted_output)
//         // infiniop::linear è¿”å›çš„æ˜¯æ–°åˆ†é…çš„ Tensorï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒæ‹·è´å› sorted_output çš„å¯¹åº”ä½ç½®
//         // è¿™ä¸€æ­¥æ˜¯ Device-to-Device Copyï¼Œé€Ÿåº¦æå¿«
        
//         float* dst_ptr = (float*)sorted_output->data() + start_idx * hidden_dim_;
//         const float* src_ptr = (const float*)expert_res->data();
//         size_t bytes = count * hidden_dim_ * sizeof(float);

//         cudaMemcpyAsync(dst_ptr, src_ptr, bytes, cudaMemcpyDeviceToDevice, stream);
//     }

//     // ======================================================================
//     // Phase 3: è¿˜åŸ (GPU Reduce)
//     // ä½¿ç”¨ sorted_row_map å’Œ sorted_weights å°†ç»“æœåŠ æƒç´¯åŠ å› final_output
//     // ======================================================================
    
//     launch_moe_reduce(
//         (float*)sorted_output->data(),
//         (int32_t*)sorted_row_map->data(),
//         (float*)sorted_weights->data(),
//         (float*)final_output->data(),
//         num_tokens, top_k, hidden_dim_, 
//         stream
//     );

//     return final_output;
// }

// // ... ä¿æŒä¸å˜ ...
// GuMoeSparseMoeBlock::GuMoeSparseMoeBlock(int num_experts, int hidden_dim, int intermediate_dim, 
//                                          int top_k, bool norm_topk, 
//                                          const DataType& dtype, const Device& device) {
//     router_ = register_module<GuMoeTopkRounter>("router", num_experts, hidden_dim, top_k, norm_topk, dtype, device);
//     experts_ = register_module<GuMoeExperts>("experts", num_experts, hidden_dim, intermediate_dim, dtype, device);
// }
// Tensor GuMoeSparseMoeBlock::forward(const Tensor& hidden_states) {
//     auto input_shape = hidden_states->shape();
//     size_t batch_size = input_shape[0];
//     size_t seq_len = input_shape[1];
//     size_t hidden_dim = input_shape[2];
//     size_t total_tokens = hidden_states->numel() / hidden_dim;
//     Tensor hidden_states_reshaped = hidden_states->view({total_tokens, hidden_dim});
//     auto [routing_weights, selected_experts] = router_->forward(hidden_states_reshaped);
//     Tensor final_hidden_states = experts_->forward(hidden_states_reshaped, selected_experts, routing_weights);
//     return final_hidden_states->view({batch_size, seq_len, hidden_dim});
// }

// } // namespace

// #include "gu_moe.h" 

// #include <cstring>
// #include <stdexcept>
// #include <vector>
// #include <cstdint> 
// #include <iostream> 
// #include <iomanip>  
// #include <cmath>
// #include <tuple> // è¡¥å……: ä¸ºäº† std::get, std::tuple

// // ç¡®ä¿åŒ…å«ä½ é¡¹ç›®ä¸­å®é™…å­˜åœ¨çš„å¤´æ–‡ä»¶
// #include "src/nvidia_kernels/nvidia_kernels_moe.h"
// #include "infinicore/ops.hpp"
// // #include "infinicore/ops/linear.hpp" // å¦‚æœ ops.hpp å·²åŒ…å«ï¼Œå¯æ³¨é‡Š
// #include "infinirt.h" 
// #include "infiniop.h" 
// #include "gu_mul.h"
// // #include "gu_mul.h" // å¦‚æœä¸éœ€è¦å¯æ³¨é‡Š
// #include "gu_topk_softmax.h" // ç¡®ä¿è¿™ä¸ªæ–‡ä»¶å­˜åœ¨

// namespace infinicore::nn {

// namespace {

// void debug_tensor(const std::string& name, const Tensor& t, int count=5) {
//     Device cpu(Device::Type::CPU);
//     Tensor c = t->to(cpu);
//     if (c->dtype() == DataType::F32) {
//         const float* ptr = reinterpret_cast<const float*>(c->data());
//         float min_v = 1e30, max_v = -1e30;
//         double sum = 0;
//         for(size_t i=0; i<c->numel(); ++i) {
//             float v = ptr[i];
//             if(v < min_v) min_v = v;
//             if(v > max_v) max_v = v;
//             sum += std::abs(v);
//         }
//         std::cout << "[DEBUG] " << name << " | Min: " << min_v << " | Max: " << max_v 
//                   << " | MeanAbs: " << (sum / c->numel()) << std::endl;
//     }
// }

// } // namespace anonymous

// // ==========================================
// // GuMoeTopkRounter å®ç°
// // ==========================================

// GuMoeTopkRounter::GuMoeTopkRounter(int num_experts, int hidden_dim, int top_k, bool norm_topk_prob, const DataType &dtype, const Device &device)
//     : top_k_(top_k), 
//       num_experts_(num_experts), 
//       hidden_dim_(hidden_dim), 
//       norm_topk_prob_(norm_topk_prob)
// {
//     infinirtSetDevice((infiniDevice_t)device.getType(), device.getIndex());
//     infiniopCreateHandle(&this->handle_);
//     // åˆå§‹åŒ–æƒé‡ï¼Œå‡è®¾å® INFINICORE_NN_PARAMETER_INIT ä¼šå¤„ç†èµ‹å€¼
//     INFINICORE_NN_PARAMETER_INIT(weight, ({ {static_cast<size_t>(num_experts_), static_cast<size_t>(hidden_dim_)}, dtype, device }));
// }

// GuMoeTopkRounter::~GuMoeTopkRounter() { 
//     if (handle_) infiniopDestroyHandle(handle_); 
// }

// std::pair<Tensor, Tensor> GuMoeTopkRounter::forward(const Tensor &hidden_states) const {
//     size_t total_tokens = hidden_states->numel() / hidden_dim_;
//     Tensor flattened = hidden_states->view({total_tokens, static_cast<size_t>(hidden_dim_)});
    
//     Tensor logits = infinicore::op::linear(flattened, weight_, std::nullopt);
    
//     auto [val, idx] = infinicore::op::topk_softmax(logits, top_k_, norm_topk_prob_, this->handle_);
    
//     return {val, idx};
// }

// // ==========================================
// // GuMoeExperts å®ç°
// // ==========================================

// GuMoeExperts::GuMoeExperts(int num_experts, int hidden_dim, int intermediate_dim, const DataType& dtype, const Device& device)
//     : num_experts_(num_experts), 
//       hidden_dim_(hidden_dim), 
//       intermediate_dim_(intermediate_dim), 
//       device_(device) 
// {
//     infinirtSetDevice((infiniDevice_t)device.getType(), device.getIndex());
//     infiniopCreateHandle(&this->handle_);
//     INFINICORE_NN_PARAMETER_INIT(gate_up_proj, ({ {static_cast<size_t>(num_experts), static_cast<size_t>(2 * intermediate_dim), static_cast<size_t>(hidden_dim)}, dtype, device }));
//     INFINICORE_NN_PARAMETER_INIT(down_proj, ({ {static_cast<size_t>(num_experts), static_cast<size_t>(hidden_dim), static_cast<size_t>(intermediate_dim)}, dtype, device }));
// }

// GuMoeExperts::~GuMoeExperts() { 
//     if (handle_) infiniopDestroyHandle(handle_); 
// }

// Tensor GuMoeExperts::forward(const Tensor& hidden_states, const Tensor& top_k_index, const Tensor& top_k_values) const {
//     if (hidden_states->dtype() != DataType::F32) throw std::runtime_error("F32 only");
    
//     // 0. ä¸Šä¸‹æ–‡å‡†å¤‡
//     Device device = hidden_states->device();
//     cudaStream_t stream = 0; // é»˜è®¤æµ

//     size_t num_tokens = hidden_states->numel() / hidden_dim_;
//     // å‡è®¾ top_k_index shape æ˜¯ [num_tokens, top_k]
//     int top_k = top_k_index->shape()[1];
//     size_t expanded_size = num_tokens * top_k;

//     // 1. åˆ†é… GPU æ˜¾å­˜ (Workspace)
//     Tensor expert_counts = Tensor::zeros({(size_t)num_experts_}, DataType::I32, device);
//     Tensor expert_offsets = Tensor::zeros({(size_t)num_experts_ + 1}, DataType::I32, device);

//     Tensor sorted_input = Tensor::empty({expanded_size, (size_t)hidden_dim_}, DataType::F32, device);
//     Tensor sorted_output = Tensor::empty({expanded_size, (size_t)hidden_dim_}, DataType::F32, device);
    
//     Tensor sorted_row_map = Tensor::empty({expanded_size}, DataType::I32, device);
//     Tensor sorted_weights = Tensor::empty({expanded_size}, DataType::F32, device);
    
//     Tensor final_output = Tensor::zeros(hidden_states->shape(), DataType::F32, device);

//     // è·å–è£¸æŒ‡é’ˆ
//     float* d_input     = (float*)hidden_states->data();
//     int32_t* d_indices = (int32_t*)top_k_index->data();
//     float* d_values    = (float*)top_k_values->data();
    
//     int32_t* d_counts  = (int32_t*)expert_counts->data();
//     int32_t* d_offsets = (int32_t*)expert_offsets->data();

//     // ======================================================================
//     // Phase 1: æ•°æ®é‡æ’ (GPU Sort & Permute)
//     // ======================================================================
    
//     launch_moe_sort(
//         d_indices, d_counts, d_offsets, 
//         num_tokens, top_k, num_experts_, 
//         stream
//     );

//     launch_moe_permute(
//         d_input, 
//         d_indices, 
//         d_values, 
//         d_offsets,
//         (float*)sorted_input->data(), 
//         (int32_t*)sorted_row_map->data(),
//         (float*)sorted_weights->data(),
//         d_counts, 
//         num_tokens, top_k, hidden_dim_, num_experts_, 
//         stream
//     );

//     // ======================================================================
//     // Phase 2: è®¡ç®— (GPU Loop)
//     // ======================================================================

//     std::vector<int32_t> h_offsets(num_experts_ + 1);
//     cudaMemcpyAsync(h_offsets.data(), d_offsets, sizeof(int32_t) * (num_experts_ + 1), cudaMemcpyDeviceToHost, stream);
//     cudaStreamSynchronize(stream); 

//     for (int e = 0; e < num_experts_; ++e) {
//         int start_idx = h_offsets[e];
//         int count = h_offsets[e+1] - start_idx;

//         if (count == 0) continue;

//         // A. åˆ‡ç‰‡ (å¦‚æœ InfiniCore ç¡®å®æ”¯æŒ narrowï¼Œè¿™é‡Œå°±æ²¡é—®é¢˜)
//         // æ³¨æ„ï¼šä¹‹å‰æŠ¥é”™è¯´æ²¡ narrowï¼Œè¿™é‡Œä¿ç•™ä½ çš„ä»£ç ã€‚å¦‚æœå†æ¬¡æŠ¥é”™ï¼Œè¯´æ˜ InfiniCore åªæœ‰ slice
//         Tensor expert_in = sorted_input->narrow({{0, (size_t)start_idx, (size_t)count}});

//         Tensor w_gate_up = gate_up_proj_->narrow({{0, (size_t)e, 1}})->view({(size_t)(2*intermediate_dim_), (size_t)hidden_dim_});
//         Tensor w_down = down_proj_->narrow({{0, (size_t)e, 1}})->view({(size_t)hidden_dim_, (size_t)intermediate_dim_});

//         // B. è®¡ç®—
//         Tensor gate_up_out = infinicore::op::linear(expert_in, w_gate_up, std::nullopt);

//         Tensor gate = gate_up_out->narrow({{1, 0, (size_t)intermediate_dim_}});
//         Tensor up = gate_up_out->narrow({{1, (size_t)intermediate_dim_, (size_t)intermediate_dim_}});
        
//         // FFN Inner
//         Tensor ffn_inner = infinicore::op::mul(infinicore::op::silu(gate), up, this->handle_);

//         Tensor expert_res = infinicore::op::linear(ffn_inner, w_down, std::nullopt);

//         // C. å†™å›
//         float* dst_ptr = (float*)sorted_output->data() + start_idx * hidden_dim_;
//         const float* src_ptr = (const float*)expert_res->data();
//         size_t bytes = count * hidden_dim_ * sizeof(float);

//         cudaMemcpyAsync(dst_ptr, src_ptr, bytes, cudaMemcpyDeviceToDevice, stream);
//     }

//     // ======================================================================
//     // Phase 3: è¿˜åŸ (GPU Reduce)
//     // ======================================================================
    
//     launch_moe_reduce(
//         (float*)sorted_output->data(),
//         (int32_t*)sorted_row_map->data(),
//         (float*)sorted_weights->data(),
//         (float*)final_output->data(),
//         num_tokens, top_k, hidden_dim_, 
//         stream
//     );

//     return final_output;
// }

// GuMoeSparseMoeBlock::GuMoeSparseMoeBlock(int num_experts, int hidden_dim, int intermediate_dim, 
//                                          int top_k, bool norm_topk, 
//                                          const DataType& dtype, const Device& device) {
//     router_ = register_module<GuMoeTopkRounter>("router", num_experts, hidden_dim, top_k, norm_topk, dtype, device);
//     experts_ = register_module<GuMoeExperts>("experts", num_experts, hidden_dim, intermediate_dim, dtype, device);
// }
// Tensor GuMoeSparseMoeBlock::forward(const Tensor& hidden_states) {
//     auto input_shape = hidden_states->shape();
//     size_t batch_size = input_shape[0];
//     size_t seq_len = input_shape[1];
//     size_t hidden_dim = input_shape[2];
//     size_t total_tokens = hidden_states->numel() / hidden_dim;
//     Tensor hidden_states_reshaped = hidden_states->view({total_tokens, hidden_dim});
//     auto [routing_weights, selected_experts] = router_->forward(hidden_states_reshaped);
//     Tensor final_hidden_states = experts_->forward(hidden_states_reshaped, selected_experts, routing_weights);
//     return final_hidden_states->view({batch_size, seq_len, hidden_dim});
// }

// } // namespace nn

// #include "gu_moe.h" 

// #include <cstring>
// #include <stdexcept>
// #include <vector>
// #include <cstdint> 
// #include <iostream> 
// #include <iomanip>  
// #include <cmath>
// #include <tuple> 

// #include "src/nvidia_kernels/nvidia_kernels_moe.h"
// #include "infinicore/ops.hpp"
// #include "infinirt.h" 
// #include "infiniop.h" 
// #include "gu_mul.h"
// #include "gu_topk_softmax.h" 

// namespace infinicore::nn {

// namespace {

// void debug_tensor(const std::string& name, const Tensor& t, int count=5) {
//     Device cpu(Device::Type::CPU);
//     Tensor c = t->to(cpu);
//     if (c->dtype() == DataType::F32) {
//         const float* ptr = reinterpret_cast<const float*>(c->data());
//         float min_v = 1e30, max_v = -1e30;
//         double sum = 0;
//         for(size_t i=0; i<c->numel(); ++i) {
//             float v = ptr[i];
//             if(v < min_v) min_v = v;
//             if(v > max_v) max_v = v;
//             sum += std::abs(v);
//         }
//         std::cout << "[DEBUG] " << name << " | Min: " << min_v << " | Max: " << max_v 
//                   << " | MeanAbs: " << (sum / c->numel()) << std::endl;
//     }
// }

// } // namespace

// // ==========================================
// // GuMoeTopkRounter å®ç°
// // ==========================================

// GuMoeTopkRounter::GuMoeTopkRounter(int num_experts, int hidden_dim, int top_k, bool norm_topk_prob, const DataType &dtype, const Device &device)
//     : top_k_(top_k), 
//       num_experts_(num_experts), 
//       hidden_dim_(hidden_dim), 
//       norm_topk_prob_(norm_topk_prob)
// {
//     infinirtSetDevice((infiniDevice_t)device.getType(), device.getIndex());
//     infiniopCreateHandle(&this->handle_);
//     INFINICORE_NN_PARAMETER_INIT(weight, ({ {static_cast<size_t>(num_experts_), static_cast<size_t>(hidden_dim_)}, dtype, device }));
// }

// GuMoeTopkRounter::~GuMoeTopkRounter() { 
//     if (handle_) infiniopDestroyHandle(handle_); 
// }

// std::pair<Tensor, Tensor> GuMoeTopkRounter::forward(const Tensor &hidden_states) const {
//     size_t total_tokens = hidden_states->numel() / hidden_dim_;
//     Tensor flattened = hidden_states->view({total_tokens, static_cast<size_t>(hidden_dim_)});
//     Tensor logits = infinicore::op::linear(flattened, weight_, std::nullopt);
//     auto [val, idx] = infinicore::op::topk_softmax(logits, top_k_, norm_topk_prob_, this->handle_);
//     return {val, idx};
// }

// // ==========================================
// // GuMoeExperts å®ç°
// // ==========================================

// GuMoeExperts::GuMoeExperts(int num_experts, int hidden_dim, int intermediate_dim, const DataType& dtype, const Device& device)
//     : num_experts_(num_experts), 
//       hidden_dim_(hidden_dim), 
//       intermediate_dim_(intermediate_dim), 
//       device_(device) 
// {
//     infinirtSetDevice((infiniDevice_t)device.getType(), device.getIndex());
//     infiniopCreateHandle(&this->handle_);
//     INFINICORE_NN_PARAMETER_INIT(gate_up_proj, ({ {static_cast<size_t>(num_experts), static_cast<size_t>(2 * intermediate_dim), static_cast<size_t>(hidden_dim)}, dtype, device }));
//     INFINICORE_NN_PARAMETER_INIT(down_proj, ({ {static_cast<size_t>(num_experts), static_cast<size_t>(hidden_dim), static_cast<size_t>(intermediate_dim)}, dtype, device }));
// }

// GuMoeExperts::~GuMoeExperts() { 
//     if (handle_) infiniopDestroyHandle(handle_); 
// }

// Tensor GuMoeExperts::forward(const Tensor& hidden_states, const Tensor& top_k_index, const Tensor& top_k_values) const {
//     if (hidden_states->dtype() != DataType::F32) throw std::runtime_error("F32 only");
    
//     Device device = hidden_states->device();
//     cudaStream_t stream = 0; 

//     size_t num_tokens = hidden_states->numel() / hidden_dim_;
//     int top_k = top_k_index->shape()[1];
//     size_t expanded_size = num_tokens * top_k;

//     auto print_shape = [](const Shape& s) {
//         std::string out = "[";
//         for(size_t i=0; i<s.size(); ++i) {
//             out += std::to_string(s[i]) + (i == s.size()-1 ? "" : ", ");
//         }
//         out += "]";
//         return out;
//     };

//     auto monitor_alloc = [&](const std::string& name, const Shape& shp, size_t unit_size) {
//         size_t total_bytes = 1;
//         for (auto s : shp) total_bytes *= s;
//         total_bytes *= unit_size;
//         std::cout << "[MEM_CHECK] Allocating [" << name << "]: Shape=" << print_shape(shp) 
//                   << ", MB=" << (total_bytes / (1024.0 * 1024.0)) << std::endl;
//     };

//     // --- Workspace åˆ†é… ---
//     monitor_alloc("expert_counts", {(size_t)num_experts_}, sizeof(int32_t));
//     Tensor expert_counts = Tensor::zeros({(size_t)num_experts_}, DataType::I32, device);

//     monitor_alloc("expert_offsets", {(size_t)num_experts_ + 1}, sizeof(int32_t));
//     Tensor expert_offsets = Tensor::zeros({(size_t)num_experts_ + 1}, DataType::I32, device);

//     monitor_alloc("sorted_input", {expanded_size, (size_t)hidden_dim_}, sizeof(float));
//     Tensor sorted_input = Tensor::empty({expanded_size, (size_t)hidden_dim_}, DataType::F32, device);

//     monitor_alloc("sorted_output", {expanded_size, (size_t)hidden_dim_}, sizeof(float));
//     Tensor sorted_output = Tensor::empty({expanded_size, (size_t)hidden_dim_}, DataType::F32, device);
    
//     monitor_alloc("sorted_row_map", {expanded_size}, sizeof(int32_t));
//     Tensor sorted_row_map = Tensor::empty({expanded_size}, DataType::I32, device);

//     monitor_alloc("sorted_weights", {expanded_size}, sizeof(float));
//     Tensor sorted_weights = Tensor::empty({expanded_size}, DataType::F32, device);
    
//     monitor_alloc("final_output", hidden_states->shape(), sizeof(float));
//     Tensor final_output = Tensor::zeros(hidden_states->shape(), DataType::F32, device);

//     float* d_input     = (float*)hidden_states->data();
//     int32_t* d_indices = (int32_t*)top_k_index->data();
//     float* d_values    = (float*)top_k_values->data();
//     int32_t* d_counts  = (int32_t*)expert_counts->data();
//     int32_t* d_offsets = (int32_t*)expert_offsets->data();

//     // Phase 1: æ’åºä¸é‡æ’ (å¢åŠ æ£€æŸ¥ç‚¹)
//     std::cout << "[CHECKPOINT] Launching moe_sort..." << std::endl;
//     launch_moe_sort(d_indices, d_counts, d_offsets, num_tokens, top_k, num_experts_, stream);
    
//     std::cout << "[CHECKPOINT] Launching moe_permute..." << std::endl;
//     launch_moe_permute(
//         d_input, d_indices, d_values, d_offsets,
//         (float*)sorted_input->data(), (int32_t*)sorted_row_map->data(), (float*)sorted_weights->data(),
//         d_counts, num_tokens, top_k, hidden_dim_, num_experts_, stream
//     );

//     // Phase 2: è®¡ç®—
//     std::vector<int32_t> h_offsets(num_experts_ + 1);
//     std::cout << "[CHECKPOINT] Copying offsets to host..." << std::endl;
//     // ä½¿ç”¨åŒæ­¥æ‹·è´ç¡®ä¿å®‰å…¨æ€§
//     cudaMemcpy(h_offsets.data(), d_offsets, sizeof(int32_t) * (num_experts_ + 1), cudaMemcpyDeviceToHost);

//     for (int e = 0; e < num_experts_; ++e) {
//         int start_idx = h_offsets[e];
//         int count = h_offsets[e+1] - start_idx;
        
//         // å¢åŠ æ•°æ®å®Œæ•´æ€§æ ¡éªŒï¼Œé˜²æ­¢ç”±äº Kernel é”™è¯¯å¯¼è‡´çš„éæ³•å†…å­˜ç”³è¯·
//         if (count < 0 || count > (int)expanded_size) {
//             std::cerr << "[FATAL] Expert " << e << " has invalid token count: " << count << std::endl;
//             continue;
//         }
//         if (count == 0) continue;

//         if (e % 20 == 0) std::cout << "[CHECKPOINT] Expert loop at " << e << ", count=" << count << std::endl;

//         Tensor expert_in = sorted_input->narrow({{0, (size_t)start_idx, (size_t)count}});
//         Tensor w_gate_up = gate_up_proj_->narrow({{0, (size_t)e, 1}})->view({(size_t)(2*intermediate_dim_), (size_t)hidden_dim_});
//         Tensor w_down = down_proj_->narrow({{0, (size_t)e, 1}})->view({(size_t)hidden_dim_, (size_t)intermediate_dim_});

//         Tensor gate_up_out = infinicore::op::linear(expert_in, w_gate_up, std::nullopt);
//         Tensor gate = gate_up_out->narrow({{1, 0, (size_t)intermediate_dim_}});
//         Tensor up = gate_up_out->narrow({{1, (size_t)intermediate_dim_, (size_t)intermediate_dim_}});
        
//         Tensor ffn_inner = infinicore::op::mul(infinicore::op::silu(gate), up, this->handle_);
//         Tensor expert_res = infinicore::op::linear(ffn_inner, w_down, std::nullopt);

//         float* dst_ptr = (float*)sorted_output->data() + start_idx * hidden_dim_;
//         cudaMemcpyAsync(dst_ptr, (float*)expert_res->data(), count * hidden_dim_ * sizeof(float), cudaMemcpyDeviceToDevice, stream);
//     }

//     // Phase 3: è¿˜åŸ
//     std::cout << "[CHECKPOINT] Launching moe_reduce..." << std::endl;
//     launch_moe_reduce(
//         (float*)sorted_output->data(), (int32_t*)sorted_row_map->data(), (float*)sorted_weights->data(),
//         (float*)final_output->data(), num_tokens, top_k, hidden_dim_, stream
//     );

//     return final_output;
// }

// // ==========================================
// // GuMoeSparseMoeBlock å®ç°
// // ==========================================

// GuMoeSparseMoeBlock::GuMoeSparseMoeBlock(int num_experts, int hidden_dim, int intermediate_dim, 
//                                          int top_k, bool norm_topk, 
//                                          const DataType& dtype, const Device& device) {
//     router_ = register_module<GuMoeTopkRounter>("router", num_experts, hidden_dim, top_k, norm_topk, dtype, device);
//     experts_ = register_module<GuMoeExperts>("experts", num_experts, hidden_dim, intermediate_dim, dtype, device);
// }

// Tensor GuMoeSparseMoeBlock::forward(const Tensor& hidden_states) {
//     auto input_shape = hidden_states->shape();
//     size_t batch_size = input_shape[0];
//     size_t seq_len = input_shape[1];
//     size_t hidden_dim = input_shape[2];
//     size_t total_tokens = hidden_states->numel() / hidden_dim;
//     Tensor hidden_states_reshaped = hidden_states->view({total_tokens, hidden_dim});
//     auto [routing_weights, selected_experts] = router_->forward(hidden_states_reshaped);
//     Tensor final_hidden_states = experts_->forward(hidden_states_reshaped, selected_experts, routing_weights);
//     return final_hidden_states->view({batch_size, seq_len, hidden_dim});
// }

// } // namespace infinicore::nn

// #include "gu_moe.h" 

// #include <cstring>
// #include <stdexcept>
// #include <vector>
// #include <cstdint> 
// #include <tuple> 

// #include "src/nvidia_kernels/nvidia_kernels_moe.h"
// #include "infinicore/ops.hpp"
// #include "infinirt.h" 
// #include "infiniop.h" 
// #include "gu_mul.h"
// #include "gu_topk_softmax.h" 

// namespace infinicore::nn {

// // ==========================================
// // GuMoeTopkRounter å®ç°
// // ==========================================

// GuMoeTopkRounter::GuMoeTopkRounter(int num_experts, int hidden_dim, int top_k, bool norm_topk_prob, const DataType &dtype, const Device &device)
//     : top_k_(top_k), 
//       num_experts_(num_experts), 
//       hidden_dim_(hidden_dim), 
//       norm_topk_prob_(norm_topk_prob)
// {
//     infinirtSetDevice((infiniDevice_t)device.getType(), device.getIndex());
//     infiniopCreateHandle(&this->handle_);
//     INFINICORE_NN_PARAMETER_INIT(weight, ({ {static_cast<size_t>(num_experts_), static_cast<size_t>(hidden_dim_)}, dtype, device }));
// }

// GuMoeTopkRounter::~GuMoeTopkRounter() { 
//     if (handle_) infiniopDestroyHandle(handle_); 
// }

// std::pair<Tensor, Tensor> GuMoeTopkRounter::forward(const Tensor &hidden_states) const {
//     size_t total_tokens = hidden_states->numel() / hidden_dim_;
//     Tensor flattened = hidden_states->view({total_tokens, static_cast<size_t>(hidden_dim_)});
//     Tensor logits = infinicore::op::linear(flattened, weight_, std::nullopt);
//     auto [val, idx] = infinicore::op::topk_softmax(logits, top_k_, norm_topk_prob_, this->handle_);
//     return {val, idx};
// }

// // ==========================================
// // GuMoeExperts å®ç°
// // ==========================================

// GuMoeExperts::GuMoeExperts(int num_experts, int hidden_dim, int intermediate_dim, const DataType& dtype, const Device& device)
//     : num_experts_(num_experts), 
//       hidden_dim_(hidden_dim), 
//       intermediate_dim_(intermediate_dim), 
//       device_(device) 
// {
//     infinirtSetDevice((infiniDevice_t)device.getType(), device.getIndex());
//     infiniopCreateHandle(&this->handle_);
//     INFINICORE_NN_PARAMETER_INIT(gate_up_proj, ({ {static_cast<size_t>(num_experts), static_cast<size_t>(2 * intermediate_dim), static_cast<size_t>(hidden_dim)}, dtype, device }));
//     INFINICORE_NN_PARAMETER_INIT(down_proj, ({ {static_cast<size_t>(num_experts), static_cast<size_t>(hidden_dim), static_cast<size_t>(intermediate_dim)}, dtype, device }));
// }

// GuMoeExperts::~GuMoeExperts() { 
//     if (handle_) infiniopDestroyHandle(handle_); 
// }

// Tensor GuMoeExperts::forward(const Tensor& hidden_states, const Tensor& top_k_index, const Tensor& top_k_values) const {
//     if (hidden_states->dtype() != DataType::F32) throw std::runtime_error("F32 only");
    
//     Device device = hidden_states->device();
//     cudaStream_t stream = 0; 

//     size_t num_tokens = hidden_states->numel() / hidden_dim_;
//     int top_k = top_k_index->shape()[1];
//     size_t expanded_size = num_tokens * top_k;

//     // 1. åˆ†é… Workspace (è¿™äº›æ˜¯æŒä¹…çš„)
//     Tensor expert_counts = Tensor::zeros({(size_t)num_experts_}, DataType::I32, device);
//     Tensor expert_offsets = Tensor::zeros({(size_t)num_experts_ + 1}, DataType::I32, device);
//     Tensor sorted_input = Tensor::empty({expanded_size, (size_t)hidden_dim_}, DataType::F32, device);
//     Tensor sorted_output = Tensor::empty({expanded_size, (size_t)hidden_dim_}, DataType::F32, device);
//     Tensor sorted_row_map = Tensor::empty({expanded_size}, DataType::I32, device);
//     Tensor sorted_weights = Tensor::empty({expanded_size}, DataType::F32, device);
//     Tensor final_output = Tensor::zeros(hidden_states->shape(), DataType::F32, device);

//     float* d_input     = (float*)hidden_states->data();
//     int32_t* d_indices = (int32_t*)top_k_index->data();
//     float* d_values    = (float*)top_k_values->data();
//     int32_t* d_counts  = (int32_t*)expert_counts->data();
//     int32_t* d_offsets = (int32_t*)expert_offsets->data();

//     launch_moe_sort(d_indices, d_counts, d_offsets, num_tokens, top_k, num_experts_, stream);
//     launch_moe_permute(
//         d_input, d_indices, d_values, d_offsets,
//         (float*)sorted_input->data(), (int32_t*)sorted_row_map->data(), (float*)sorted_weights->data(),
//         d_counts, num_tokens, top_k, hidden_dim_, num_experts_, stream
//     );

//     // 2. æ‹·è´ Offset å¿…é¡»åŒæ­¥ï¼Œå¦åˆ™åé¢å¾ªç¯ä¼šä¹±
//     std::vector<int32_t> h_offsets(num_experts_ + 1);
//     cudaMemcpy(h_offsets.data(), d_offsets, sizeof(int32_t) * (num_experts_ + 1), cudaMemcpyDeviceToHost);

//     // 3. ä¸“å®¶å¾ªç¯ï¼šä½¿ç”¨å¤§æ‹¬å·æ§åˆ¶å±€éƒ¨å˜é‡ç”Ÿå‘½å‘¨æœŸ
//     for (int e = 0; e < num_experts_; ++e) {
//         int start_idx = h_offsets[e];
//         int count = h_offsets[e+1] - start_idx;
//         if (count <= 0) continue;

//         {
//             // åœ¨è¿™ä¸ªå¤§æ‹¬å·å†…å®šä¹‰çš„ Tensor ä¼šåœ¨æ¯ä¸€è½®è¿­ä»£ç»“æŸæ—¶ç«‹å³ææ„
//             // è¿™èƒ½å¼ºåˆ¶è®© cudaMallocAsync çŸ¥é“è¿™å—å†…å­˜å¯ä»¥å›æ”¶äº†
//             Tensor expert_in = sorted_input->narrow({{0, (size_t)start_idx, (size_t)count}});
//             Tensor w_gate_up = gate_up_proj_->narrow({{0, (size_t)e, 1}})->view({(size_t)(2*intermediate_dim_), (size_t)hidden_dim_});
//             Tensor w_down = down_proj_->narrow({{0, (size_t)e, 1}})->view({(size_t)hidden_dim_, (size_t)intermediate_dim_});

//             Tensor gate_up_out = infinicore::op::linear(expert_in, w_gate_up, std::nullopt);
//             Tensor gate = gate_up_out->narrow({{1, 0, (size_t)intermediate_dim_}});
//             Tensor up = gate_up_out->narrow({{1, (size_t)intermediate_dim_, (size_t)intermediate_dim_}});
            
//             Tensor ffn_inner = infinicore::op::mul(infinicore::op::silu(gate), up, this->handle_);
//             Tensor expert_res = infinicore::op::linear(ffn_inner, w_down, std::nullopt);

//             float* dst_ptr = (float*)sorted_output->data() + start_idx * hidden_dim_;
//             cudaMemcpyAsync(dst_ptr, (float*)expert_res->data(), count * hidden_dim_ * sizeof(float), cudaMemcpyDeviceToDevice, stream);
//         } // <--- å…³é”®ï¼šåœ¨è¿™é‡Œï¼Œä¸Šä¸€è½®çš„æ‰€æœ‰ä¸­é—´ Tensor éƒ½ä¼šè¢«é‡Šæ”¾
//     }

//     launch_moe_reduce(
//         (float*)sorted_output->data(), (int32_t*)sorted_row_map->data(), (float*)sorted_weights->data(),
//         (float*)final_output->data(), num_tokens, top_k, hidden_dim_, stream
//     );

//     // 4. æœ€ç»ˆåŒæ­¥ï¼šè§£å†³å…¨é›¶é—®é¢˜çš„å…³é”®
//     cudaStreamSynchronize(stream);

//     return final_output;
// }

// // ==========================================
// // GuMoeSparseMoeBlock å®ç°
// // ==========================================

// GuMoeSparseMoeBlock::GuMoeSparseMoeBlock(int num_experts, int hidden_dim, int intermediate_dim, 
//                                          int top_k, bool norm_topk, 
//                                          const DataType& dtype, const Device& device) {
//     router_ = register_module<GuMoeTopkRounter>("router", num_experts, hidden_dim, top_k, norm_topk, dtype, device);
//     experts_ = register_module<GuMoeExperts>("experts", num_experts, hidden_dim, intermediate_dim, dtype, device);
// }

// Tensor GuMoeSparseMoeBlock::forward(const Tensor& hidden_states) {
//     auto input_shape = hidden_states->shape();
//     size_t batch_size = input_shape[0];
//     size_t seq_len = input_shape[1];
//     size_t hidden_dim = input_shape[2];
//     size_t total_tokens = hidden_states->numel() / hidden_dim;
//     Tensor hidden_states_reshaped = hidden_states->view({total_tokens, hidden_dim});
//     auto [routing_weights, selected_experts] = router_->forward(hidden_states_reshaped);
//     Tensor final_hidden_states = experts_->forward(hidden_states_reshaped, selected_experts, routing_weights);
//     return final_hidden_states->view({batch_size, seq_len, hidden_dim});
// }

//} // namespace infinicore::nn

// #include "gu_moe.h" 
// #include <cstring>
// #include <stdexcept>
// #include <vector>
// #include <cstdint> 
// #include <tuple> 

// #include "src/nvidia_kernels/nvidia_kernels_moe.h"
// #include "infinicore/ops.hpp"
// #include "infinirt.h" 
// #include "infiniop.h" 
// #include "gu_mul.h"
// #include "gu_topk_softmax.h" 

// // å°è¯•å¼•å…¥æ¡†æ¶çš„æµè·å–æ¥å£
// namespace infinicore::context {
//     extern void* getStream();
// }

// namespace infinicore::nn {

// // ==========================================
// // GuMoeTopkRounter
// // ==========================================
// GuMoeTopkRounter::GuMoeTopkRounter(int num_experts, int hidden_dim, int top_k, bool norm_topk_prob, const DataType &dtype, const Device &device)
//     : top_k_(top_k), num_experts_(num_experts), hidden_dim_(hidden_dim), norm_topk_prob_(norm_topk_prob) {
//     infinirtSetDevice((infiniDevice_t)device.getType(), device.getIndex());
//     infiniopCreateHandle(&this->handle_);
//     INFINICORE_NN_PARAMETER_INIT(weight, ({ {static_cast<size_t>(num_experts_), static_cast<size_t>(hidden_dim_)}, dtype, device }));
// }

// GuMoeTopkRounter::~GuMoeTopkRounter() { if (handle_) infiniopDestroyHandle(handle_); }

// std::pair<Tensor, Tensor> GuMoeTopkRounter::forward(const Tensor &hidden_states) const {
//     size_t total_tokens = hidden_states->numel() / hidden_dim_;
//     Tensor flattened = hidden_states->view({total_tokens, static_cast<size_t>(hidden_dim_)});
//     Tensor logits = infinicore::op::linear(flattened, weight_, std::nullopt);
//     auto [val, idx] = infinicore::op::topk_softmax(logits, top_k_, norm_topk_prob_, this->handle_);
//     return {val, idx};
// }

// // ==========================================
// // GuMoeExperts
// // ==========================================
// GuMoeExperts::GuMoeExperts(int num_experts, int hidden_dim, int intermediate_dim, const DataType& dtype, const Device& device)
//     : num_experts_(num_experts), hidden_dim_(hidden_dim), intermediate_dim_(intermediate_dim), device_(device) {
//     infinirtSetDevice((infiniDevice_t)device.getType(), device.getIndex());
//     infiniopCreateHandle(&this->handle_);
//     INFINICORE_NN_PARAMETER_INIT(gate_up_proj, ({ {static_cast<size_t>(num_experts), static_cast<size_t>(2 * intermediate_dim), static_cast<size_t>(hidden_dim)}, dtype, device }));
//     INFINICORE_NN_PARAMETER_INIT(down_proj, ({ {static_cast<size_t>(num_experts), static_cast<size_t>(hidden_dim), static_cast<size_t>(intermediate_dim)}, dtype, device }));
// }

// GuMoeExperts::~GuMoeExperts() { if (handle_) infiniopDestroyHandle(handle_); }

// Tensor GuMoeExperts::forward(const Tensor& hidden_states, const Tensor& top_k_index, const Tensor& top_k_values) const {
//     Device device = hidden_states->device();
//     // ä½¿ç”¨æ¡†æ¶æµï¼Œå¦‚æœæ²¡æœ‰åˆ™é€€å›åˆ°é»˜è®¤æµ 0
//     void* raw_stream = infinicore::context::getStream();
//     cudaStream_t stream = (cudaStream_t)raw_stream; //? (cudaStream_t)raw_stream : (cudaStream_t)0;

//     size_t num_tokens = hidden_states->numel() / hidden_dim_;
//     int top_k = top_k_index->shape()[1];
//     size_t expanded_size = num_tokens * top_k;

//     // åˆ†é… Workspace
//     Tensor expert_counts = Tensor::zeros({(size_t)num_experts_}, DataType::I32, device);
//     Tensor expert_offsets = Tensor::zeros({(size_t)num_experts_ + 1}, DataType::I32, device);
//     Tensor sorted_input = Tensor::empty({expanded_size, (size_t)hidden_dim_}, DataType::F32, device);
//     Tensor sorted_output = Tensor::empty({expanded_size, (size_t)hidden_dim_}, DataType::F32, device);
//     Tensor sorted_row_map = Tensor::empty({expanded_size}, DataType::I32, device);
//     Tensor sorted_weights = Tensor::empty({expanded_size}, DataType::F32, device);
//     Tensor final_output = Tensor::zeros(hidden_states->shape(), DataType::F32, device);

//     // Phase 1
//     launch_moe_sort((int32_t*)top_k_index->data(), (int32_t*)expert_counts->data(), (int32_t*)expert_offsets->data(), num_tokens, top_k, num_experts_, stream);
//     launch_moe_permute((float*)hidden_states->data(), (int32_t*)top_k_index->data(), (float*)top_k_values->data(), (int32_t*)expert_offsets->data(),
//                        (float*)sorted_input->data(), (int32_t*)sorted_row_map->data(), (float*)sorted_weights->data(),
//                        (int32_t*)expert_counts->data(), num_tokens, top_k, hidden_dim_, num_experts_, stream);

//     // Phase 2
//     std::vector<int32_t> h_offsets(num_experts_ + 1);
//     cudaMemcpy(h_offsets.data(), expert_offsets->data(), sizeof(int32_t) * (num_experts_ + 1), cudaMemcpyDeviceToHost);

//     for (int e = 0; e < num_experts_; ++e) {
//         int start_idx = h_offsets[e];
//         int count = h_offsets[e+1] - start_idx;
//         if (count <= 0) continue;

//         { // å±€éƒ¨ä½œç”¨åŸŸå›æ”¶æ˜¾å­˜
//             Tensor expert_in = sorted_input->narrow({{0, (size_t)start_idx, (size_t)count}});
//             Tensor w_gate_up = gate_up_proj_->narrow({{0, (size_t)e, 1}})->view({(size_t)(2*intermediate_dim_), (size_t)hidden_dim_});
//             Tensor w_down = down_proj_->narrow({{0, (size_t)e, 1}})->view({(size_t)hidden_dim_, (size_t)intermediate_dim_});

//             Tensor gate_up_out = infinicore::op::linear(expert_in, w_gate_up, std::nullopt);
//             Tensor gate = gate_up_out->narrow({{1, 0, (size_t)intermediate_dim_}});
//             Tensor up = gate_up_out->narrow({{1, (size_t)intermediate_dim_, (size_t)intermediate_dim_}});
//             Tensor ffn_inner = infinicore::op::mul(infinicore::op::silu(gate), up, this->handle_);
//             Tensor expert_res = infinicore::op::linear(ffn_inner, w_down, std::nullopt);

//             cudaMemcpyAsync((float*)sorted_output->data() + start_idx * hidden_dim_, (float*)expert_res->data(), count * hidden_dim_ * sizeof(float), cudaMemcpyDeviceToDevice, stream);
//         }
//     }

//     // Phase 3
//     launch_moe_reduce((float*)sorted_output->data(), (int32_t*)sorted_row_map->data(), (float*)sorted_weights->data(), (float*)final_output->data(), num_tokens, top_k, hidden_dim_, stream);
    
//     cudaStreamSynchronize(stream);
//     return final_output;
// }

// // ==========================================
// // GuMoeSparseMoeBlock
// // ==========================================
// GuMoeSparseMoeBlock::GuMoeSparseMoeBlock(int num_experts, int hidden_dim, int intermediate_dim, int top_k, bool norm_topk, const DataType& dtype, const Device& device) {
//     router_ = register_module<GuMoeTopkRounter>("router", num_experts, hidden_dim, top_k, norm_topk, dtype, device);
//     experts_ = register_module<GuMoeExperts>("experts", num_experts, hidden_dim, intermediate_dim, dtype, device);
// }

// Tensor GuMoeSparseMoeBlock::forward(const Tensor& hidden_states) {
//     size_t total_tokens = hidden_states->numel() / (hidden_states->shape().back());
//     Tensor hidden_states_reshaped = hidden_states->view({total_tokens, hidden_states->shape().back()});
//     auto [routing_weights, selected_experts] = router_->forward(hidden_states_reshaped);
//     Tensor final_hidden_states = experts_->forward(hidden_states_reshaped, selected_experts, routing_weights);
//     return final_hidden_states->view(hidden_states->shape());
// }

// } // namespace infinicore::nn

// #include "gu_moe.h" 
// #include <cstring>
// #include <stdexcept>
// #include <vector>
// #include <cstdint> 
// #include <tuple> 
// #include <iostream>

// #include "src/nvidia_kernels/nvidia_kernels_moe.h"
// #include "infinicore/ops.hpp"
// #include "infinirt.h" 
// #include "infiniop.h" 
// #include "gu_mul.h"
// #include "gu_topk_softmax.h" 

// namespace infinicore::nn {

// // ==========================================
// // GuMoeTopkRounter
// // ==========================================
// GuMoeTopkRounter::GuMoeTopkRounter(int num_experts, int hidden_dim, int top_k, bool norm_topk_prob, const DataType &dtype, const Device &device)
//     : top_k_(top_k), num_experts_(num_experts), hidden_dim_(hidden_dim), norm_topk_prob_(norm_topk_prob) {
//     infinirtSetDevice((infiniDevice_t)device.getType(), device.getIndex());
//     infiniopCreateHandle(&this->handle_);
//     INFINICORE_NN_PARAMETER_INIT(weight, ({ {static_cast<size_t>(num_experts_), static_cast<size_t>(hidden_dim_)}, dtype, device }));
// }

// GuMoeTopkRounter::~GuMoeTopkRounter() { if (handle_) infiniopDestroyHandle(handle_); }

// std::pair<Tensor, Tensor> GuMoeTopkRounter::forward(const Tensor &hidden_states) const {
//     size_t total_tokens = hidden_states->numel() / hidden_dim_;
//     Tensor flattened = hidden_states->view({total_tokens, static_cast<size_t>(hidden_dim_)});
//     Tensor logits = infinicore::op::linear(flattened, weight_, std::nullopt);
//     auto [val, idx] = infinicore::op::topk_softmax(logits, top_k_, norm_topk_prob_, this->handle_);
//     return {val, idx};
// }

// // ==========================================
// // GuMoeExperts
// // ==========================================
// GuMoeExperts::GuMoeExperts(int num_experts, int hidden_dim, int intermediate_dim, const DataType& dtype, const Device& device)
//     : num_experts_(num_experts), 
//       hidden_dim_(hidden_dim), 
//       intermediate_dim_(intermediate_dim), 
//       device_(device) 
// {
//     // --- å¢åŠ è¿™ä¸€æ®µå¼ºåŠ›æ‰“å° ---
//     printf("\n[CONSTRUCTOR_DEBUG] num_experts: %d, hidden: %d, inter: %d\n", 
//            num_experts, hidden_dim, intermediate_dim);
//     fflush(stdout); 

//     if (num_experts <= 0 || hidden_dim <= 0 || intermediate_dim <= 0) {
//         printf("[FATAL] Invalid dimensions detected!\n");
//         fflush(stdout);
//     }
//     // -------------------------

//     infinirtSetDevice((infiniDevice_t)device.getType(), device.getIndex());
//     infiniopCreateHandle(&this->handle_);

//     INFINICORE_NN_PARAMETER_INIT(gate_up_proj, ({ {static_cast<size_t>(num_experts), static_cast<size_t>(2 * intermediate_dim), static_cast<size_t>(hidden_dim)}, dtype, device }));
//     INFINICORE_NN_PARAMETER_INIT(down_proj, ({ {static_cast<size_t>(num_experts), static_cast<size_t>(hidden_dim), static_cast<size_t>(intermediate_dim)}, dtype, device }));
// }

// GuMoeExperts::~GuMoeExperts() { if (handle_) infiniopDestroyHandle(handle_); }

// Tensor GuMoeExperts::forward(const Tensor& hidden_states, const Tensor& top_k_index, const Tensor& top_k_values) const {
//     Device device = hidden_states->device();
//     cudaStream_t stream = 0; 

//     size_t num_tokens = hidden_states->numel() / hidden_dim_;
//     int top_k = top_k_index->shape()[1];
//     size_t expanded_size = (size_t)num_tokens * top_k;

//     // 1. æ˜¾å¼åˆ†é… Workspace
//     Tensor expert_counts = Tensor::zeros({(size_t)num_experts_}, DataType::I32, device);
//     Tensor expert_offsets = Tensor::zeros({(size_t)num_experts_ + 1}, DataType::I32, device);
//     Tensor sorted_input = Tensor::empty({expanded_size, (size_t)hidden_dim_}, DataType::F32, device);
//     Tensor sorted_output = Tensor::empty({expanded_size, (size_t)hidden_dim_}, DataType::F32, device);
//     Tensor sorted_row_map = Tensor::empty({expanded_size}, DataType::I32, device);
//     Tensor sorted_weights = Tensor::empty({expanded_size}, DataType::F32, device);
//     Tensor final_output = Tensor::zeros(hidden_states->shape(), DataType::F32, device);

//     // Phase 1: æ•°æ®é‡æ’
//     launch_moe_sort((int32_t*)top_k_index->data(), (int32_t*)expert_counts->data(), (int32_t*)expert_offsets->data(), (int)num_tokens, top_k, num_experts_, stream);
//     launch_moe_permute((float*)hidden_states->data(), (int32_t*)top_k_index->data(), (float*)top_k_values->data(), (int32_t*)expert_offsets->data(),
//                        (float*)sorted_input->data(), (int32_t*)sorted_row_map->data(), (float*)sorted_weights->data(),
//                        (int32_t*)expert_counts->data(), (int)num_tokens, top_k, hidden_dim_, num_experts_, stream);

//     // Phase 2: è®¡ç®—å¾ªç¯
//     std::vector<int32_t> h_offsets(num_experts_ + 1);
//     cudaMemcpy(h_offsets.data(), expert_offsets->data(), sizeof(int32_t) * (num_experts_ + 1), cudaMemcpyDeviceToHost);

//     for (int e = 0; e < num_experts_; ++e) {
//         int start_idx = h_offsets[e];
//         int count = h_offsets[e+1] - start_idx;
//         if (count <= 0) continue;

//         { // åˆ©ç”¨ä½œç”¨åŸŸè‡ªåŠ¨ææ„ä¸´æ—¶ Tensorï¼Œé‡Šæ”¾æ˜¾å­˜æ± 
//             Tensor expert_in = sorted_input->narrow({{0, (size_t)start_idx, (size_t)count}});
//             Tensor w_gate_up = gate_up_proj_->narrow({{0, (size_t)e, 1}})->view({(size_t)(2*intermediate_dim_), (size_t)hidden_dim_});
//             Tensor w_down = down_proj_->narrow({{0, (size_t)e, 1}})->view({(size_t)hidden_dim_, (size_t)intermediate_dim_});

//             // æ‰§è¡Œ FFN
//             Tensor gate_up_out = infinicore::op::linear(expert_in, w_gate_up, std::nullopt);
//             Tensor gate = gate_up_out->narrow({{1, 0, (size_t)intermediate_dim_}});
//             Tensor up = gate_up_out->narrow({{1, (size_t)intermediate_dim_, (size_t)intermediate_dim_}});
            
//             Tensor activated_gate = infinicore::op::silu(gate);
//             Tensor ffn_inner = infinicore::op::mul(activated_gate, up, this->handle_);
//             Tensor expert_res = infinicore::op::linear(ffn_inner, w_down, std::nullopt);

//             cudaMemcpyAsync((float*)sorted_output->data() + start_idx * hidden_dim_, (float*)expert_res->data(), (size_t)count * hidden_dim_ * sizeof(float), cudaMemcpyDeviceToDevice, stream);
//         } // æ­¤å¤„å±€éƒ¨ Tensor è‡ªåŠ¨ææ„
//     }

//     // Phase 3: ç»“æœè§„çº¦
//     launch_moe_reduce((float*)sorted_output->data(), (int32_t*)sorted_row_map->data(), (float*)sorted_weights->data(), (float*)final_output->data(), (int)num_tokens, top_k, hidden_dim_, stream);
    
//     cudaStreamSynchronize(stream);
//     return final_output;
// }

// // ==========================================
// // GuMoeSparseMoeBlock
// // ==========================================
// GuMoeSparseMoeBlock::GuMoeSparseMoeBlock(int num_experts, int hidden_dim, int intermediate_dim, int top_k, bool norm_topk, const DataType& dtype, const Device& device) {
//     router_ = register_module<GuMoeTopkRounter>("router", num_experts, hidden_dim, top_k, norm_topk, dtype, device);
//     experts_ = register_module<GuMoeExperts>("experts", num_experts, hidden_dim, intermediate_dim, dtype, device);
// }

// Tensor GuMoeSparseMoeBlock::forward(const Tensor& hidden_states) {
//     auto shp = hidden_states->shape();
//     size_t last_dim = shp.back();
//     size_t total_tokens = hidden_states->numel() / last_dim;
    
//     Tensor hidden_states_reshaped = hidden_states->view({total_tokens, last_dim});
//     auto [routing_weights, selected_experts] = router_->forward(hidden_states_reshaped);
//     Tensor final_hidden_states = experts_->forward(hidden_states_reshaped, selected_experts, routing_weights);
    
//     return final_hidden_states->view(shp);
// }

// } // namespace infinicore::nn

// #include "gu_moe.h" 
// #include <cstring>
// #include <stdexcept>
// #include <vector>
// #include <cstdint> 
// #include <tuple> 
// #include <iostream>

// #include "src/nvidia_kernels/nvidia_kernels_moe.h"
// #include "infinicore/ops.hpp"
// #include "infinirt.h" 
// #include "infiniop.h" 
// #include "gu_mul.h"
// #include "gu_topk_softmax.h" 

// // å¼•å…¥æ¡†æ¶æµæ¥å£
// namespace infinicore::context {
//     extern void* getStream();
// }

// namespace infinicore::nn {

// // GuMoeTopkRounter (ä¿æŒä¸å˜)
// GuMoeTopkRounter::GuMoeTopkRounter(int num_experts, int hidden_dim, int top_k, bool norm_topk_prob, const DataType &dtype, const Device &device)
//     : top_k_(top_k), num_experts_(num_experts), hidden_dim_(hidden_dim), norm_topk_prob_(norm_topk_prob) {
//     infinirtSetDevice((infiniDevice_t)device.getType(), device.getIndex());
//     infiniopCreateHandle(&this->handle_);
//     INFINICORE_NN_PARAMETER_INIT(weight, ({ {static_cast<size_t>(num_experts_), static_cast<size_t>(hidden_dim_)}, dtype, device }));
// }
// GuMoeTopkRounter::~GuMoeTopkRounter() { if (handle_) infiniopDestroyHandle(handle_); }
// std::pair<Tensor, Tensor> GuMoeTopkRounter::forward(const Tensor &hidden_states) const {
//     size_t total_tokens = hidden_states->numel() / hidden_dim_;
//     Tensor flattened = hidden_states->view({total_tokens, static_cast<size_t>(hidden_dim_)});
//     Tensor logits = infinicore::op::linear(flattened, weight_, std::nullopt);
//     auto [val, idx] = infinicore::op::topk_softmax(logits, top_k_, norm_topk_prob_, this->handle_);
//     return {val, idx};
// }

// // GuMoeExperts (ä¿æŒä¸å˜)
// GuMoeExperts::GuMoeExperts(int num_experts, int hidden_dim, int intermediate_dim, const DataType& dtype, const Device& device)
//     : num_experts_(num_experts), hidden_dim_(hidden_dim), intermediate_dim_(intermediate_dim), device_(device) {
//     infinirtSetDevice((infiniDevice_t)device.getType(), device.getIndex());
//     infiniopCreateHandle(&this->handle_);
//     INFINICORE_NN_PARAMETER_INIT(gate_up_proj, ({ {static_cast<size_t>(num_experts), static_cast<size_t>(2 * intermediate_dim), static_cast<size_t>(hidden_dim)}, dtype, device }));
//     INFINICORE_NN_PARAMETER_INIT(down_proj, ({ {static_cast<size_t>(num_experts), static_cast<size_t>(hidden_dim), static_cast<size_t>(intermediate_dim)}, dtype, device }));
// }
// GuMoeExperts::~GuMoeExperts() { if (handle_) infiniopDestroyHandle(handle_); }

// Tensor GuMoeExperts::forward(const Tensor& hidden_states, const Tensor& top_k_index, const Tensor& top_k_values) const {
//     Device device = hidden_states->device();
//     void* raw_stream = infinicore::context::getStream();
//     cudaStream_t stream = raw_stream ? (cudaStream_t)raw_stream : 0;

//     // å›é€€ç±»å‹è½¬æ¢ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æŒ‡é’ˆï¼Œä½†åœ¨ count è®¡ç®—å¤„åšé˜²å¾¡
//     size_t num_tokens = hidden_states->numel() / hidden_dim_;
//     int top_k = top_k_index->shape()[1];
//     size_t expanded_size = num_tokens * top_k;

//     Tensor expert_counts = Tensor::zeros({(size_t)num_experts_}, DataType::I32, device);
//     Tensor expert_offsets = Tensor::zeros({(size_t)num_experts_ + 1}, DataType::I32, device);
//     Tensor sorted_input = Tensor::empty({expanded_size, (size_t)hidden_dim_}, DataType::F32, device);
//     Tensor sorted_output = Tensor::empty({expanded_size, (size_t)hidden_dim_}, DataType::F32, device);
//     Tensor sorted_row_map = Tensor::empty({expanded_size}, DataType::I32, device);
//     Tensor sorted_weights = Tensor::empty({expanded_size}, DataType::F32, device);
//     Tensor final_output = Tensor::zeros(hidden_states->shape(), DataType::F32, device);

//     launch_moe_sort(
//         (int32_t*)top_k_index->data(), 
//         (int32_t*)expert_counts->data(), 
//         (int32_t*)expert_offsets->data(), 
//         (int)num_tokens, top_k, num_experts_, stream
//     );
    
//     launch_moe_permute(
//         (float*)hidden_states->data(), 
//         (int32_t*)top_k_index->data(), 
//         (float*)top_k_values->data(), 
//         (int32_t*)expert_offsets->data(),
//         (float*)sorted_input->data(), 
//         (int32_t*)sorted_row_map->data(), 
//         (float*)sorted_weights->data(),
//         (int32_t*)expert_counts->data(), 
//         (int)num_tokens, top_k, hidden_dim_, num_experts_, stream
//     );

//     std::vector<int32_t> h_offsets(num_experts_ + 1);
//     cudaMemcpy(h_offsets.data(), expert_offsets->data(), sizeof(int32_t) * (num_experts_ + 1), cudaMemcpyDeviceToHost);

//     for (int e = 0; e < num_experts_; ++e) {
//         int start_idx = h_offsets[e];
//         int count = h_offsets[e+1] - start_idx;
        
//         // ã€æ ¸å¿ƒé˜²å¾¡ã€‘é˜²æ­¢ Error 700 / OOM
//         // å¦‚æœ count å¼‚å¸¸ï¼ˆå¯èƒ½æ˜¯ç”±äº Int64/32 è¯»å–é”™ä½å¯¼è‡´çš„ï¼‰ï¼Œç›´æ¥è·³è¿‡ï¼
//         if (count <= 0 || count > (int)expanded_size) {
//             if (count > (int)expanded_size) {
//                 printf("WARNING: Expert %d skipped due to invalid count: %d\n", e, count);
//             }
//             printf("WARNING: Expert %d skipped due to invalid count: %d\n", e, count);
//             continue;
//         }

//         { 
//             Tensor expert_in = sorted_input->narrow({{0, (size_t)start_idx, (size_t)count}});
//             Tensor w_gate_up = gate_up_proj_->narrow({{0, (size_t)e, 1}})->view({(size_t)(2*intermediate_dim_), (size_t)hidden_dim_});
//             Tensor w_down = down_proj_->narrow({{0, (size_t)e, 1}})->view({(size_t)hidden_dim_, (size_t)intermediate_dim_});

//             Tensor gate_up_out = infinicore::op::linear(expert_in, w_gate_up, std::nullopt);
//             Tensor gate = gate_up_out->narrow({{1, 0, (size_t)intermediate_dim_}});
//             Tensor up = gate_up_out->narrow({{1, (size_t)intermediate_dim_, (size_t)intermediate_dim_}});
            
//             Tensor ffn_inner = infinicore::op::mul(infinicore::op::silu(gate), up, this->handle_);
//             Tensor expert_res = infinicore::op::linear(ffn_inner, w_down, std::nullopt);

//             cudaMemcpyAsync((float*)sorted_output->data() + start_idx * hidden_dim_, (float*)expert_res->data(), (size_t)count * hidden_dim_ * sizeof(float), cudaMemcpyDeviceToDevice, stream);
//         }
//     }

//     launch_moe_reduce((float*)sorted_output->data(), (int32_t*)sorted_row_map->data(), (float*)sorted_weights->data(), (float*)final_output->data(), (int)num_tokens, top_k, hidden_dim_, stream);
    
//     cudaStreamSynchronize(stream);
//     return final_output;
// }

// // GuMoeSparseMoeBlock (ä¿æŒä¸å˜)
// GuMoeSparseMoeBlock::GuMoeSparseMoeBlock(int num_experts, int hidden_dim, int intermediate_dim, int top_k, bool norm_topk, const DataType& dtype, const Device& device) {
//     router_ = register_module<GuMoeTopkRounter>("router", num_experts, hidden_dim, top_k, norm_topk, dtype, device);
//     experts_ = register_module<GuMoeExperts>("experts", num_experts, hidden_dim, intermediate_dim, dtype, device);
// }
// Tensor GuMoeSparseMoeBlock::forward(const Tensor& hidden_states) {
//     size_t total_tokens = hidden_states->numel() / (hidden_states->shape().back());
//     Tensor hidden_states_reshaped = hidden_states->view({total_tokens, hidden_states->shape().back()});
//     auto [routing_weights, selected_experts] = router_->forward(hidden_states_reshaped);
//     Tensor final_hidden_states = experts_->forward(hidden_states_reshaped, selected_experts, routing_weights);
//     return final_hidden_states->view(hidden_states->shape());
// }

// } // namespace infinicore::nn

// #include <cuda_runtime.h>
// #include <device_launch_parameters.h>
// #include <cstdio>
// #include <cub/cub.cuh>

// #define MAX_EXPERTS 256

// #define CUDA_CHECK(call) \
// do { \
//     cudaError_t error = call; \
//     if (error != cudaSuccess) { \
//         fprintf(stderr, "CUDA Error at line %d: %s\n", __LINE__, cudaGetErrorString(error)); \
//         exit(1); \
//     } \
// } while(0)

// __global__ void count_kernel_sota(
//     const int32_t* __restrict__ topk_ids, 
//     int32_t* __restrict__ expert_counts,  
//     int total_tasks,
//     int num_experts
// ) {
//     extern __shared__ int32_t smem_counts[]; 
    
//     int tid = threadIdx.x;
//     int bid = blockIdx.x;
//     int gid = bid * blockDim.x + tid;

//     for (int i = tid; i < num_experts; i += blockDim.x) {
//         smem_counts[i] = 0;
//     }
//     __syncthreads();

//     if (gid < total_tasks) {
//         int expert_id = topk_ids[gid];

//         unsigned int active_mask = __activemask();
//         unsigned int mask = __match_any_sync(active_mask, expert_id);

//         int leader = __ffs(mask) - 1; // Find First Set
//         int lane_id = tid % 32;

//         if (lane_id == leader) {

//             int agg_count = __popc(mask);
            
//             atomicAdd(&smem_counts[expert_id], agg_count);
//         }
//     }
    
//     __syncthreads();

//     for (int i = tid; i < num_experts; i += blockDim.x) {
//         int count = smem_counts[i];
//         if (count > 0) {
//             atomicAdd(&expert_counts[i], count);
//         }
//     }
// }

// void launch_moe_sort(
//     const int32_t* topk_ids,
//     int32_t* expert_counts,   
//     int32_t* expert_offsets, // é•¿åº¦å»ºè®®æ˜¯ num_experts + 1
//     int num_tokens,
//     int top_k,
//     int num_experts,
//     cudaStream_t stream
// ) {
//     int total_tasks = num_tokens * top_k;
//     int block_size = 256;
//     int grid_size = (total_tasks + block_size - 1) / block_size;

//     // -------------------------------------------------------
//     CUDA_CHECK(cudaMemsetAsync(expert_counts, 0, num_experts * sizeof(int32_t), stream));
    
//     count_kernel_sota<<<grid_size, block_size, num_experts * sizeof(int32_t), stream>>>(
//         topk_ids, expert_counts, total_tasks, num_experts
//     );
    
//     void* d_temp_storage = NULL;
//     size_t temp_storage_bytes = 0;
    
//     cub::DeviceScan::ExclusiveSum(d_temp_storage, temp_storage_bytes, 
//                                   expert_counts,  // è¾“å…¥: counts
//                                   expert_offsets, // è¾“å‡º: offsets
//                                   num_experts + 1,// é•¿åº¦: å¤šç®—ä¸€ä½ä½œä¸ºæ€»å’Œ
//                                   stream);
    
//     CUDA_CHECK(cudaMallocAsync(&d_temp_storage, temp_storage_bytes, stream));
    
//     // æ‰§è¡Œ
//     cub::DeviceScan::ExclusiveSum(d_temp_storage, temp_storage_bytes, 
//                                   expert_counts, 
//                                   expert_offsets, 
//                                   num_experts + 1, 
//                                   stream);
                                  
//     CUDA_CHECK(cudaFreeAsync(d_temp_storage, stream));
// }

// __global__ void permute_kernel(
//     const float* __restrict__ input,           // [N, H] æºæ•°æ®
//     const int32_t* __restrict__ topk_ids,      // [N, K] è·¯ç”±
//     const float* __restrict__ topk_weights,
//     const int32_t* __restrict__ expert_offsets,// [E]    èµ·å§‹ä½ç½®
//     int32_t* __restrict__ running_counters,    // [E]    ä¸´æ—¶è®¡æ•°å™¨ (åŸå­åŠ ä¸“ç”¨)
//     float* __restrict__ sorted_input,          // [N*K, H] ç›®æ ‡æ•°æ®
//     int32_t* __restrict__ sorted_row_map,      // [N*K]    æ¥æºè®°å½•
//     float* __restrict__ sorted_weights,
//     int num_tokens,
//     int top_k,
//     int hidden_dim
// ) {
//     // ä»»åŠ¡æ€»æ•° = Tokenæ•° * TopK (å› ä¸ºå¯èƒ½æœ‰å¤åˆ¶)
//     int total_tasks = num_tokens * top_k;
//     int tid = blockIdx.x * blockDim.x + threadIdx.x;
    
//     if (tid >= total_tasks) return;

//     int token_idx = tid / top_k;
//     // int k_idx = tid % top_k; // å¦‚æœ weights æ˜¯ [N, K] å¸ƒå±€ï¼Œéœ€è¦ç”¨è¿™ä¸ª
//     int expert_id = topk_ids[tid];

//     // è·å–è¯¥ä¸“å®¶çš„èµ·å§‹åœ°å€
//     int base_offset = expert_offsets[expert_id];
//     // åŸå­è·å–æˆ‘æ˜¯è¯¥ä¸“å®¶çš„ç¬¬å‡ ä¸ªå®¢äºº
//     int my_rank = atomicAdd(&running_counters[expert_id], 1);
//     // è®¡ç®—æœ€ç»ˆå†™å…¥çš„è¡Œå·
//     int target_row = base_offset + my_rank;

//     // è®°ä¸‹ï¼šç¬¬ target_row è¡Œæ•°æ®ï¼Œå…¶å®æ˜¯åŸæ¥çš„ token_idx
//     sorted_row_map[target_row] = token_idx;
//     sorted_weights[target_row] = topk_weights[tid];

//     // ä» input[token_idx] æ¬åˆ° sorted_input[target_row]
//     const float* src_ptr = input + token_idx * hidden_dim;
//     float* dst_ptr = sorted_input + target_row * hidden_dim;

//     // å°è¯•ä½¿ç”¨ float4 (128-bit) è¿›è¡Œæ¬è¿ï¼Œå‡å°‘æŒ‡ä»¤æ•°
//     int vec_size = hidden_dim / 4;
//     int remainder = hidden_dim % 4;
    
//     // å¼ºè½¬æŒ‡é’ˆè¿›è¡Œå‘é‡åŒ–è¯»å–
//     const float4* src_vec = (const float4*)src_ptr;
//     float4* dst_vec = (float4*)dst_ptr;

//     for (int i = 0; i < vec_size; ++i) {
//         dst_vec[i] = src_vec[i];
//     }
//     // å¤„ç†å‰©ä¸‹çš„å°¾å·´ (å¦‚æœæœ‰çš„è¯)
//     for (int i = 0; i < remainder; ++i) {
//         int idx = vec_size * 4 + i;
//         dst_ptr[idx] = src_ptr[idx];
//     }
// }

// void launch_moe_permute(
//     const float* input,
//     const int32_t* topk_ids,
//     const float* topk_weights,
//     const int32_t* expert_offsets,
//     float* sorted_input,
//     int32_t* sorted_row_map,
//     float* sorted_weights,
//     int32_t* expert_counts, // <--- å¤ç”¨è¿™ä¸ªæ•°ç»„ä½œä¸ºä¸´æ—¶è®¡æ•°å™¨
//     int num_tokens,
//     int top_k,
//     int hidden_dim,
//     int num_experts,
//     cudaStream_t stream
// ) {
//     int total_tasks = num_tokens * top_k;
//     int block_size = 256;
//     int grid_size = (total_tasks + block_size - 1) / block_size;

//     // 1. ã€å…³é”®ã€‘æŠŠè®¡æ•°å™¨é‡ç½®ä¸º 0
//     // è¿™æ ·æ¯ä¸ªä¸“å®¶æ‰èƒ½ä»ç¬¬ 0 ä¸ªå¼€å§‹æ•°
//     CUDA_CHECK(cudaMemsetAsync(expert_counts, 0, num_experts * sizeof(int32_t), stream));

//     // 2. å¯åŠ¨ Kernel
//     permute_kernel<<<grid_size, block_size, 0, stream>>>(
//         input, 
//         topk_ids, 
//         topk_weights,
//         expert_offsets, 
//         expert_counts, // è¿™é‡Œä¼ è¿›å»å½“ä½œ running_counters ç”¨
//         sorted_input, 
//         sorted_row_map,
//         sorted_weights,
//         num_tokens, 
//         top_k, 
//         hidden_dim
//     );
// }

// #include <cuda_runtime.h>
// #include <device_launch_parameters.h>
// #include <cstdio>
// #include <cub/cub.cuh>

// #define MAX_EXPERTS 256

// // å¢å¼ºç‰ˆ Check å®
// #define CUDA_CHECK(call) \
// do { \
//     cudaError_t error = call; \
//     if (error != cudaSuccess) { \
//         fprintf(stderr, "[KERNEL ERROR] %s failed at line %d: %s\n", #call, __LINE__, cudaGetErrorString(error)); \
//         exit(1); \
//     } \
// } while(0)

// // =============================================================
// // 1. Count Kernel (ç»Ÿè®¡æ¯ä¸ªä¸“å®¶çš„ token æ•°)
// // =============================================================
// __global__ void count_kernel_sota(
//     const int32_t* __restrict__ topk_ids, 
//     int32_t* __restrict__ expert_counts,  
//     int total_tasks,
//     int num_experts
// ) {
//     extern __shared__ int32_t smem_counts[]; 
    
//     int tid = threadIdx.x;
//     int bid = blockIdx.x;
//     int gid = bid * blockDim.x + tid;

//     // åˆå§‹åŒ–å…±äº«å†…å­˜
//     for (int i = tid; i < num_experts; i += blockDim.x) {
//         smem_counts[i] = 0;
//     }
//     __syncthreads();

//     // ç»Ÿè®¡
//     if (gid < total_tasks) {
//         int expert_id = topk_ids[gid];
//         // ç®€å•çš„è¾¹ç•Œæ£€æŸ¥
//         if (expert_id >= 0 && expert_id < num_experts) {
//             unsigned int active_mask = __activemask();
//             unsigned int mask = __match_any_sync(active_mask, expert_id);
//             int leader = __ffs(mask) - 1; 
//             int lane_id = tid % 32;
//             if (lane_id == leader) {
//                 int agg_count = __popc(mask);
//                 atomicAdd(&smem_counts[expert_id], agg_count);
//             }
//         }
//     }
//     __syncthreads();

//     // å†™å›å…¨å±€å†…å­˜
//     for (int i = tid; i < num_experts; i += blockDim.x) {
//         int count = smem_counts[i];
//         if (count > 0) {
//             atomicAdd(&expert_counts[i], count);
//         }
//     }
// }

// void launch_moe_sort(
//     const int32_t* topk_ids,
//     int32_t* expert_counts,   
//     int32_t* expert_offsets, 
//     int num_tokens,
//     int top_k,
//     int num_experts,
//     cudaStream_t stream
// ) {
//     int total_tasks = num_tokens * top_k;
//     int block_size = 256;
//     int grid_size = (total_tasks + block_size - 1) / block_size;

//     // æ¸…é›¶ Counts
//     CUDA_CHECK(cudaMemsetAsync(expert_counts, 0, num_experts * sizeof(int32_t), stream));
    
//     // è¿è¡Œç»Ÿè®¡
//     count_kernel_sota<<<grid_size, block_size, num_experts * sizeof(int32_t), stream>>>(
//         topk_ids, expert_counts, total_tasks, num_experts
//     );
    
//     // CUB Scan (å‰ç¼€å’Œ)
//     void* d_temp_storage = NULL;
//     size_t temp_storage_bytes = 0;
    
//     // æŸ¥è¯¢æ‰€éœ€æ˜¾å­˜ (æ³¨æ„ num_experts + 1 ä»¥è®¡ç®—æ€»å’Œ)
//     // è¿™é‡Œçš„ expert_counts å¯¹åº” gumoe.cpp é‡Œç”³è¯·çš„ (num_experts + 1) å¤§å°ï¼Œå®‰å…¨ã€‚
//     cub::DeviceScan::ExclusiveSum(d_temp_storage, temp_storage_bytes, 
//                                   expert_counts, expert_offsets, 
//                                   num_experts + 1, stream);
    
//     // ====================================================
//     // ã€å…³é”®ä¿®æ”¹ã€‘ä½¿ç”¨åŒæ­¥ cudaMalloc
//     // å¿…é¡»æ›¿æ¢æ‰åŸæ¥çš„ cudaMallocAsyncï¼Œå¦åˆ™åœ¨ä½ çš„ç¯å¢ƒé‡Œä¼šåˆ†é…å¤±è´¥å¯¼è‡´ Core Dump
//     // ====================================================
//     CUDA_CHECK(cudaMalloc(&d_temp_storage, temp_storage_bytes));
    
//     // æ‰§è¡Œ Scan
//     cub::DeviceScan::ExclusiveSum(d_temp_storage, temp_storage_bytes, 
//                                   expert_counts, expert_offsets, 
//                                   num_experts + 1, stream);
                                  
//     // åŒæ­¥é‡Šæ”¾
//     CUDA_CHECK(cudaFree(d_temp_storage));
// }

// // =============================================================
// // 2. Permute Kernel (é‡æ’æ•°æ®)
// // =============================================================
// __global__ void permute_kernel(
//     const float* __restrict__ input,           
//     const int32_t* __restrict__ topk_ids,      
//     const float* __restrict__ topk_weights,
//     const int32_t* __restrict__ expert_offsets,
//     int32_t* __restrict__ running_counters,    
//     float* __restrict__ sorted_input,          
//     int32_t* __restrict__ sorted_row_map,      
//     float* __restrict__ sorted_weights,
//     int num_tokens,
//     int top_k,
//     int hidden_dim
// ) {
//     int total_tasks = num_tokens * top_k;
//     int tid = blockIdx.x * blockDim.x + threadIdx.x;
    
//     if (tid >= total_tasks) return;

//     int token_idx = tid / top_k;
//     int expert_id = topk_ids[tid];

//     // åŸå­è·å–å†™å…¥ä½ç½®
//     int base_offset = expert_offsets[expert_id];
//     int my_rank = atomicAdd(&running_counters[expert_id], 1);
//     int target_row = base_offset + my_rank;

//     // è®°å½•æ˜ å°„å…³ç³»
//     if (sorted_row_map) sorted_row_map[target_row] = token_idx;
//     if (sorted_weights) sorted_weights[target_row] = topk_weights[tid];

//     // æ¬è¿ Hidden States
//     const float* src_ptr = input + token_idx * hidden_dim;
//     float* dst_ptr = sorted_input + target_row * hidden_dim;

//     // ç®€å•çš„ float4 ä¼˜åŒ–
//     int vec_size = hidden_dim / 4;
//     int remainder = hidden_dim % 4;
//     const float4* src_vec = (const float4*)src_ptr;
//     float4* dst_vec = (float4*)dst_ptr;

//     for (int i = 0; i < vec_size; ++i) {
//         dst_vec[i] = src_vec[i];
//     }
//     for (int i = 0; i < remainder; ++i) {
//         int idx = vec_size * 4 + i;
//         dst_ptr[idx] = src_ptr[idx];
//     }
// }

// void launch_moe_permute(
//     const float* input,
//     const int32_t* topk_ids,
//     const float* topk_weights,
//     const int32_t* expert_offsets,
//     float* sorted_input,
//     int32_t* sorted_row_map,
//     float* sorted_weights,
//     int32_t* expert_counts, 
//     int num_tokens,
//     int top_k,
//     int hidden_dim,
//     int num_experts,
//     cudaStream_t stream
// ) {
//     int total_tasks = num_tokens * top_k;
//     int block_size = 256;
//     int grid_size = (total_tasks + block_size - 1) / block_size;

//     // å¤ç”¨ expert_counts ä½œä¸ºè®¡æ•°å™¨ï¼Œå¿…é¡»æ¸…é›¶
//     CUDA_CHECK(cudaMemsetAsync(expert_counts, 0, (num_experts + 1)* sizeof(int32_t), stream));

//     permute_kernel<<<grid_size, block_size, 0, stream>>>(
//         input, topk_ids, topk_weights, expert_offsets, expert_counts, 
//         sorted_input, sorted_row_map, sorted_weights,
//         num_tokens, top_k, hidden_dim
//     );
// }

// #include <cuda_runtime.h>
// #include <device_launch_parameters.h>
// #include <cstdio>
// #include <cub/cub.cuh>
// #include <vector>
// #define MAX_EXPERTS 256

// // é”™è¯¯æ£€æŸ¥å®
// #define CUDA_CHECK(call) \
// do { \
//     cudaError_t error = call; \
//     if (error != cudaSuccess) { \
//         fprintf(stderr, "[KERNEL ERROR] %s failed at line %d: %s\n", #call, __LINE__, cudaGetErrorString(error)); \
//         exit(1); \
//     } \
// } while(0)

// // ==========================================================
// // ã€æ–°æ­¦å™¨ã€‘GPU æ•°æ®æ¢é’ˆ
// // ==========================================================
// __global__ void debug_inspector(int32_t* counts, int32_t* offsets, int num_experts) {
//     if (threadIdx.x == 0 && blockIdx.x == 0) {
//         printf("\n[GPU INSPECTOR] --- Start Analysis ---\n");
        
//         // 1. æ£€æŸ¥ Counts (å‰10ä¸ª)
//         printf("[GPU] Counts (First 10): ");
//         bool counts_all_zero = true;
//         for(int i=0; i<min(10, num_experts); ++i) {
//             printf("%d ", counts[i]);
//             if (counts[i] != 0) counts_all_zero = false;
//         }
//         printf("\n");

//         // 2. æ£€æŸ¥ Offsets (å‰10ä¸ª å’Œ æœ€åä¸€ä¸ª)
//         printf("[GPU] Offsets (First 10): ");
//         for(int i=0; i<min(10, num_experts); ++i) printf("%d ", offsets[i]);
//         printf("... Last(Total): %d\n", offsets[num_experts]);

//         // 3. å®æ—¶è¯Šæ–­
//         if (offsets[0] > 1000000000 || offsets[0] < 0) {
//             printf("[GPU CRITICAL] Offsets[0] is garbage! CUB Scan failed.\n");
//         }
//         if (counts_all_zero && offsets[num_experts] == 0) {
//             printf("[GPU WARNING] Counts are all zero. Input indices might be wrong.\n");
//         }
//         printf("[GPU INSPECTOR] --- End Analysis ---\n\n");
//     }
// }

// // ----------------------------------------------------------
// // Count Kernel (ä¿æŒä¸å˜)
// // ----------------------------------------------------------
// __global__ void count_kernel_sota(
//     const int32_t* __restrict__ topk_ids, 
//     int32_t* __restrict__ expert_counts,  
//     int total_tasks,
//     int num_experts
// ) {
//     extern __shared__ int32_t smem_counts[]; 
//     int tid = threadIdx.x;
//     int gid = blockIdx.x * blockDim.x + tid;
//     if (gid == 0) {
//         printf("[GPU ALIVE] Kernel started. total_tasks=%d, num_experts=%d\n", total_tasks, num_experts);
//         printf("[GPU DATA] First topk_id = %d\n", topk_ids[0]); 
//     }
//     for (int i = tid; i < num_experts; i += blockDim.x) smem_counts[i] = 0;
//     __syncthreads();

//     if (gid < total_tasks) {
//         int expert_id = topk_ids[gid];
//         if (expert_id >= 0 && expert_id < num_experts) {
//             unsigned int mask = __match_any_sync(__activemask(), expert_id);
//             if ((tid % 32) == (__ffs(mask) - 1)) {
//                 atomicAdd(&smem_counts[expert_id], __popc(mask));
//             }
//         }
//     }
//     __syncthreads();
//     for (int i = tid; i < num_experts; i += blockDim.x) {
//         if (smem_counts[i] > 0) atomicAdd(&expert_counts[i], smem_counts[i]);
//         // printf("è¿™æ˜¯count_kernel_sotaçš„æ•°å­—%d\n", smem_counts[i]);
//     }
// }

// // ----------------------------------------------------------
// // Sort Launch (æ¤å…¥äº†æ¢é’ˆ)
// // ----------------------------------------------------------
// // void launch_moe_sort(
// //     const int32_t* topk_ids,
// //     int32_t* expert_counts,   
// //     int32_t* expert_offsets, 
// //     int num_tokens,
// //     int top_k,
// //     int num_experts,
// //     cudaStream_t stream
// // ) {
// //     int total_tasks = num_tokens * top_k;
// //     int block_size = 256;
// //     int grid_size = (total_tasks + block_size - 1) / block_size;
// //     printf("6\n");
// //     // æ¸…é›¶ (æ³¨æ„ï¼šè¿™é‡Œç”¨åŒæ­¥ memset ä»¥æ’é™¤å¼‚æ­¥å¹²æ‰°)
// //     CUDA_CHECK(cudaMemset(expert_counts, 0, (num_experts + 1) * sizeof(int32_t)));
    
// //     count_kernel_sota<<<grid_size, block_size, num_experts * sizeof(int32_t), stream>>>(
// //         topk_ids, expert_counts, total_tasks, num_experts
// //     );
// //     printf("7\n");
// //     // CUB Scan
// //     void* d_temp_storage = NULL;
// //     size_t temp_storage_bytes = 0;
    
// //     cub::DeviceScan::ExclusiveSum(d_temp_storage, temp_storage_bytes, 
// //                                   expert_counts, expert_offsets, 
// //                                   num_experts + 1, stream);
// //     printf("8\n");
// //     // ã€å¼ºåˆ¶åŒæ­¥åˆ†é…ã€‘ç¡®ä¿ Scan å†…å­˜ç»å¯¹å¯ç”¨
// //     CUDA_CHECK(cudaMalloc(&d_temp_storage, temp_storage_bytes));
    
// //     cub::DeviceScan::ExclusiveSum(d_temp_storage, temp_storage_bytes, 
// //                                   expert_counts, expert_offsets, 
// //                                   num_experts + 1, stream);
// //     printf("9\n");
// //     CUDA_CHECK(cudaFree(d_temp_storage));
// // }
// void launch_moe_sort(
//     const int32_t* topk_ids,
//     int32_t* expert_counts,   
//     int32_t* expert_offsets, 
//     int num_tokens,
//     int top_k,
//     int num_experts,
//     cudaStream_t stream
// ) {
//     int total_tasks = num_tokens * top_k;
//     int block_size = 256;
//     int grid_size = (total_tasks + block_size - 1) / block_size;

//     printf("6 - Preparing to launch count_kernel\n");
    
//     // 1. æ¸…é›¶
//     CUDA_CHECK(cudaMemsetAsync(expert_counts, 0, (num_experts + 1) * sizeof(int32_t), stream));
    
//     // 2. è®¡ç®—å…±äº«å†…å­˜å¤§å° (å…³é”®ï¼)
//     size_t smem_size = (num_experts + 1) * sizeof(int32_t);
    
//     // [DEBUG] æ‰“å°å¯åŠ¨å‚æ•°ï¼Œçœ‹çœ‹æ˜¯ä¸æ˜¯æœ‰ 0
//     printf(">>> Launch Params: Grid=%d, Block=%d, SharedMem=%zu bytes, Experts=%d\n", 
//            grid_size, block_size, smem_size, num_experts);

//     // 3. å¯åŠ¨ Kernel
//     count_kernel_sota<<<grid_size, block_size, smem_size, stream>>>(
//         topk_ids, expert_counts, total_tasks, num_experts
//     );

//     // =========================================================
//     // ã€æ•è·å¯åŠ¨å¤±è´¥ã€‘è¿™æ˜¯ä½ æ²¡çœ‹åˆ° printf çš„çœŸæ­£åŸå› 
//     // =========================================================
//     cudaError_t launch_err = cudaGetLastError();
//     if (launch_err != cudaSuccess) {
//         printf("âŒ [FATAL] Kernel Launch Failed! Code=%d, Msg=%s\n", 
//                launch_err, cudaGetErrorString(launch_err));
//         // è¿™é‡Œä¸è¦ exitï¼Œæ‰“å°å‡ºæ¥è®©æˆ‘ä»¬çœ‹åˆ°åŸå› 
//     } else {
//         printf("âœ… Kernel Launch Requested Successfully.\n");
//     }

//     // 4. CUB Scan (ä¿æŒä½ ç°åœ¨çš„ä»£ç )
//     void* d_temp_storage = NULL;
//     size_t temp_storage_bytes = 0;
    
//     cub::DeviceScan::ExclusiveSum(d_temp_storage, temp_storage_bytes, 
//                                   expert_counts, expert_offsets, 
//                                   num_experts + 1, stream);
    
//     CUDA_CHECK(cudaMalloc(&d_temp_storage, temp_storage_bytes));
    
//     cub::DeviceScan::ExclusiveSum(d_temp_storage, temp_storage_bytes, 
//                                   expert_counts, expert_offsets, 
//                                   num_experts + 1, stream);
                                  
//     CUDA_CHECK(cudaFree(d_temp_storage));
// }

// // ----------------------------------------------------------
// // Permute Kernel (ä¿æŒä¸å˜)
// // ----------------------------------------------------------
// __global__ void permute_kernel(
//     const float* __restrict__ input,           
//     const int32_t* __restrict__ topk_ids,      
//     const float* __restrict__ topk_weights,
//     const int32_t* __restrict__ expert_offsets,
//     int32_t* __restrict__ running_counters,    
//     float* __restrict__ sorted_input,          
//     int32_t* __restrict__ sorted_row_map,      
//     float* __restrict__ sorted_weights,
//     int num_tokens,
//     int top_k,
//     int hidden_dim
// ) {
//     int total_tasks = num_tokens * top_k;
//     int tid = blockIdx.x * blockDim.x + threadIdx.x;
    
//     if (tid >= total_tasks) return;

//     int token_idx = tid / top_k;
//     int expert_id = topk_ids[tid];

//     int base_offset = expert_offsets[expert_id];
//     int my_rank = atomicAdd(&running_counters[expert_id], 1);
//     int target_row = base_offset + my_rank;

//     if (sorted_row_map) sorted_row_map[target_row] = token_idx;
//     if (sorted_weights) sorted_weights[target_row] = topk_weights[tid];

//     const float* src_ptr = input + token_idx * hidden_dim;
//     float* dst_ptr = sorted_input + target_row * hidden_dim;

//     for (int i = 0; i < hidden_dim; ++i) dst_ptr[i] = src_ptr[i];
// }

// void launch_moe_permute(
//     const float* input,
//     const int32_t* topk_ids,
//     const float* topk_weights,
//     const int32_t* expert_offsets,
//     float* sorted_input,
//     int32_t* sorted_row_map,
//     float* sorted_weights,
//     int32_t* expert_counts, 
//     int num_tokens,
//     int top_k,
//     int hidden_dim,
//     int num_experts,
//     cudaStream_t stream
// ) {
//     int block_size = 256;
//     int grid_size = (num_tokens * top_k + block_size - 1) / block_size;

//     // æ¸…é›¶ running_counters
//     CUDA_CHECK(cudaMemset(expert_counts, 0, (num_experts + 1) * sizeof(int32_t)));

//     permute_kernel<<<grid_size, block_size, 0, stream>>>(
//         input, topk_ids, topk_weights, expert_offsets, expert_counts, 
//         sorted_input, sorted_row_map, sorted_weights,
//         num_tokens, top_k, hidden_dim
//     );
// }


// # import os
// # import torch
// # # ã€å¼ºåˆ¶é»‘é­”æ³•ã€‘å‘Šè¯‰ PyTorch æˆ‘ä»¬è¦ç”¨æ–°ç‰ˆ ABI
// # # è¿™è¡Œä»£ç èƒ½æ•‘å‘½ï¼Œå®ƒé˜²æ­¢ PyTorch è‡ªå·±æŠŠ flag æ”¹å› 0
// # torch._C._GLIBCXX_USE_CXX11_ABI = True

// # from setuptools import setup
// # from torch.utils.cpp_extension import BuildExtension, CUDAExtension
// # import pybind11

// # INFINI_SRC_ROOT = "/data/users/shankgu/InfiniCore" 
// # INFINI_LM_ROOT = "/data/users/shankgu/InfiniLM"
// # INFINI_LIB_DIR = "/data/users/shankgu/InfiniCore/build/linux/x86_64/release"

// # # ä½ çš„åº“åˆ—è¡¨ (ä¿æŒä½ ä¹‹å‰çš„é…ç½®)
// # libs = [
// #     # å¦‚æœä½ çš„ gumoe.cpp ç»§æ‰¿äº† Moduleï¼Œä½ éœ€è¦é“¾æ¥ utils åº“
// #     os.path.join(INFINI_LIB_DIR, 'libinfini-utils.a'), 
// #     os.path.join(INFINI_LIB_DIR, 'libinfiniop-nvidia.a'),
// #     os.path.join(INFINI_LIB_DIR, 'libinfiniccl-nvidia.a'),
// #     os.path.join(INFINI_LIB_DIR, 'libinfinirt-nvidia.a') 
// # ]

// # setup(
// #     name='gu_moe_ops',
// #     version='0.1.0',
// #     ext_modules=[
// #         CUDAExtension(
// #             name='gu_moe_ops',
// #             sources=[
// #                 'pybind_gumoe.cc',          
// #                 'src/gumoe.cpp',            
// #                 'src/gu_mul.cc',            
// #                 'src/gu_topk_softmax.cc',
// #                 'src/nvidia_kernels/gu_reduce.cu',
// #                 'src/nvidia_kernels/gu_sort.cu',    
// #             ],
// #             include_dirs=[
// #                 pybind11.get_include(),
// #                 os.path.join(INFINI_SRC_ROOT, 'include'),
// #                 os.path.join(INFINI_LM_ROOT, 'src'),
// #                 'src'                       
// #             ],
// #             extra_objects=libs,
            
// #             extra_compile_args={
// #                 # ã€å”¯ä¸€å…³é”®ç‚¹ã€‘å¿…é¡»è®¾ä¸º 1ï¼Œè§£å†³ ...ESs æŠ¥é”™
// #                 'cxx': ['-O3', '-std=c++17', '-D_GLIBCXX_USE_CXX11_ABI=1'],
// #                 'nvcc': ['-O3']
// #             }
// #         )
// #     ],
// #     cmdclass={
// #         'build_ext': BuildExtension
// #     }
// # )

// #include <cuda_runtime.h>
// #include <pybind11/pybind11.h>
// #include <pybind11/stl.h>
// #include <pybind11/numpy.h>
// #include <vector>
// #include <iostream>

// #include "src/gu_moe.h"
// #include "infinicore/tensor.hpp"
// #include "infinicore/device.hpp"

// namespace py = pybind11;
// using namespace infinicore;

// // int_to_dtype ç•¥ (ä¿æŒä¸å˜)...
// infinicore::DataType int_to_dtype(int id) {
//     switch (id) {
//         case 0: return infinicore::DataType::F32;
//         case 1: return infinicore::DataType::BF16;
//         case 2: return infinicore::DataType::I32;
//         case 3: return infinicore::DataType::F16;
//         default: throw std::runtime_error("Unknown dtype id: " + std::to_string(id));
//     }
// }

// class PyGuMoeWrapper {
// public:
//     std::shared_ptr<nn::GuMoeSparseMoeBlock> block;

//     PyGuMoeWrapper(int num_experts, int hidden_dim, int intermediate_dim, 
//                    int dtype_id, int device_id) {
//         Device device(Device::Type::NVIDIA, device_id);
//         block = std::make_shared<nn::GuMoeSparseMoeBlock>(
//             num_experts, hidden_dim, intermediate_dim, 2, true, 
//             int_to_dtype(dtype_id), device
//         );
//     }

//     infinicore::nn::Parameter object_to_tensor(py::object tensor_obj) {
//         uint64_t ptr_val = tensor_obj.attr("ptr").cast<uint64_t>();
//         void* raw_ptr = reinterpret_cast<void*>(ptr_val);
//         std::vector<int64_t> shape_vec = tensor_obj.attr("shape").cast<std::vector<int64_t>>();
//         infinicore::Shape shape;
//         for(auto s : shape_vec) shape.push_back(s);
//         int dtype_id = tensor_obj.attr("dtype_id").cast<int>();
//         int dev_id = tensor_obj.attr("device_id").cast<int>();
//         infinicore::Device dev(infinicore::Device::Type::NVIDIA, dev_id);
//         return infinicore::Tensor::from_blob(raw_ptr, shape, int_to_dtype(dtype_id), dev);
//     }

//     // âœ…âœ…âœ… ã€å…³é”®ã€‘è¿™é‡Œåªæœ‰ 2 ä¸ªå‚æ•°ï¼
//     void forward(py::object input_obj, py::object output_obj) {
//         auto input = object_to_tensor(input_obj);
        
//         // è°ƒç”¨ C++ Block (å•å‚æ•° forward)
//         auto internal_result = block->forward(input);
        
//         auto output_buffer = object_to_tensor(output_obj);
//         size_t bytes = internal_result->numel() * 4;
//         cudaMemcpy(output_buffer->data(), internal_result->data(), bytes, cudaMemcpyDeviceToDevice);
//     }

//     // âœ… set_weights æ¥æ”¶ 3 ä¸ªå‚æ•°ï¼šGateUp, Down, RouterWeight
//     void set_weights(py::object gate_up_obj, py::object down_obj, py::object router_w_obj) {
//         auto gate_up = object_to_tensor(gate_up_obj);
//         auto down = object_to_tensor(down_obj);
//         auto router_w = object_to_tensor(router_w_obj);
        
//         block->set_weights(gate_up, down, router_w);
//         std::cout << "[C++] All weights set (Experts + Router)." << std::endl;
//     }
// };

// PYBIND11_MODULE(gu_moe_ops, m) {
//     py::class_<PyGuMoeWrapper>(m, "GuMoeBlock")
//         .def(py::init<int, int, int, int, int>()) 
//         .def("forward", &PyGuMoeWrapper::forward)
//         .def("set_weights", &PyGuMoeWrapper::set_weights);
// }